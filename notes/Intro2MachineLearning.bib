
@book{vaart_asymptotic_1998,
	address = {Cambridge, UK ; New York, NY, USA},
	title = {Asymptotic {Statistics}},
	isbn = {9780521496032},
	abstract = {Here is a practical and mathematically rigorous introduction to the field of asymptotic statistics. In addition to most of the standard topics of an asymptotics course--likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures--the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, one of the book's unifying themes that mainly entails the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	month = oct,
	year = {1998},
	file = {Asymptotic Statistics [van der Vaart].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/URPF9ZVW/Asymptotic Statistics [van der Vaart].pdf:application/pdf}
}

@book{abramovich_statistical_2013,
	title = {Statistical theory: a concise introduction},
	isbn = {9781439851845  1439851840},
	shorttitle = {Statistical theory},
	abstract = {"Designed for a one-semester advanced undergraduate or graduate course, Statistical Theory: A Concise Introduction clearly explains the underlying ideas and principles of major statistical concepts, including parameter estimation, confidence intervals, hypothesis testing, asymptotic analysis, Bayesian inference, and elements of decision theory. It introduces these topics on a clear intuitive level using illustrative examples in addition to the formal definitions, theorems, and proofs. Based on the authors' lecture notes, this student-oriented, self-contained book maintains a proper balance between the clarity and rigor of exposition. In a few cases, the authors present a 'sketched' version of a proof, explaining its main ideas rather than giving detailed technical mathematical and probabilistic arguments. Chapters and sections marked by asterisks contain more advanced topics and may be omitted. A special chapter on linear models shows how the main theoretical concepts can be applied to the well-known and frequently used statistical tool of linear regression.Requiring no heavy calculus, simple questions throughout the text help students check their understanding of the material. Each chapter also includes a set of exercises that range in level of difficulty"-- "Preface This book is intended as a textbook for a one-term course in statistical theory for advanced undergraduates in statistics, mathematics or other related fields although at least parts of it may be useful for graduates as well. Although there exist many good books on the topic, having taught a one-term Statistical Theory course during the years we felt that it is somewhat hard to recommend a particular one as a proper textbook to undergraduate students in statistics. Some of the existing textbooks with a primary focus on rigorous formalism, in our view, do not explain sufficiently clearly the underlying ideas and principles of the main statistical concepts, and are more suitable for graduates. Some others are "all-inclusive" textbooks that include a variety of topics in statistics that make them "too heavy" for a one-term course in statistical theory. Our main motivation was to propose a more "student-oriented" self-contained textbook designed for a one-term course on statistical theory that would introduce basic statistical concepts first on a clear intuitive level with illustrative examples in addition to the (necessary!) formal definitions, theorems and proofs. It is based on our lecture notes. We tried to keep a proper balance between the clarity and rigorousness of exposition. In a few cases we preferred to present a "sketched" version of a proof explaining its main ideas or even to give it up at all rather then to follow detailed technical mathematical and probabilistic arguments. The interested reader can complete those proofs from other existing books on mathematical statistics (see the bibliography)"--},
	language = {English},
	author = {Abramovich, Felix and Ritov, Ya'acov},
	year = {2013}
}

@book{hastie_elements_2003,
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {0387952845},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics.

Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry.

The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.

FROM THE REVIEWS:

TECHNOMETRICS "[This] is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
	urldate = {2014-12-11},
	publisher = {Springer},
	author = {Hastie, T and Tibshirani, R and Friedman, JH},
	month = jul,
	year = {2003},
	keywords = {machine-learning, statistic}
}

@article{goldenberg_survey_2010,
	title = {A {Survey} of {Statistical} {Network} {Models}},
	volume = {2},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000005},
	doi = {10.1561/2200000005},
	abstract = {Networks are ubiquitous in science and have become a focal point for discussion in everyday life. Formal statistical models for the analysis of network data have emerged as a major topic of interest in diverse areas of study, and most of these involve a form of graphical representation. Probability models on graphs date back to 1959. Along with empirical studies in social psychology and sociology from the 1960s, these early works generated an active "network community" and a substantial literature in the 1970s. This effort moved into the statistical literature in the late 1970s and 1980s, and the past decade has seen a burgeoning network literature in statistical physics and computer science. The growth of the World Wide Web and the emergence of online "networking communities" such as Facebook, MySpace, and LinkedIn, and a host of more specialized professional network communities has intensified interest in the study of networks and network data. Our goal in this review is to provide the reader with an entry point to this burgeoning literature. We begin with an overview of the historical development of statistical network modeling and then we introduce a number of examples that have been studied in the network literature. Our subsequent discussion focuses on a number of prominent static and dynamic network models and their interconnections. We emphasize formal model descriptions, and pay special attention to the interpretation of parameters and their estimation. We end with a description of some open problems and challenges for machine learning and statistics.},
	number = {2},
	urldate = {2013-04-03},
	journal = {Found. Trends Mach. Learn.},
	author = {Goldenberg, A. and Zheng, A.X. and Fienberg, S.E. and Airoldi, E.M.},
	month = feb,
	year = {2010},
	pages = {129--233},
	file = {10.1.1.169.2632.pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/U5XIB8NC/10.1.1.169.2632.pdf:application/pdf}
}

@book{james_introduction_2013,
	address = {New York},
	edition = {1st ed. 2013. Corr. 4th printing 2014 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {9781461471370},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = aug,
	year = {2013}
}

@book{mccullagh_generalized_1989,
	address = {Boca Raton},
	edition = {2 edition},
	title = {Generalized {Linear} {Models}, {Second} {Edition}},
	isbn = {9780412317606},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {McCullagh, P. and Nelder, John A.},
	month = aug,
	year = {1989}
}

@book{vapnik_statistical_1998,
	address = {New York},
	edition = {1 edition},
	title = {Statistical {Learning} {Theory}},
	isbn = {9780471030034},
	abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Vapnik, Vladimir N.},
	month = sep,
	year = {1998}
}

@article{schmidhuber_deep_2015,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep {Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2015-04-08},
	journal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	month = jan,
	year = {2015},
	note = {arXiv: 1404.7828},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {85--117},
	file = {arXiv\:1404.7828 PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PSV772F4/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/RG8VU9J2/1404.html:text/html}
}

@book{shalev-shwartz_understanding_2014,
	address = {New York, NY},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {9781107057135},
	shorttitle = {Understanding {Machine} {Learning}},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	month = may,
	year = {2014}
}

@article{abramovich_derivation_1999,
	title = {Derivation of equivalent kernel for general spline smoothing: a systematic approach},
	volume = {5},
	issn = {1350-7265},
	shorttitle = {Derivation of equivalent kernel for general spline smoothing},
	url = {http://projecteuclid.org/euclid.bj/1173147911},
	abstract = {We consider first the spline smoothing nonparametric estimation with variable smoothing parameter and arbitrary design density function and show that the corresponding equivalent kernel can be approximated by the Green function of a certain linear differential operator. Furthermore, we propose to use the standard (in applied mathematics and engineering) method for asymptotic solution of linear differential equations, known as the Wentzel-Kramers-Brillouin method, for systematic derivation of an asymptotically equivalent kernel in this general case. The corresponding results for polynomial splines are a special case of the general solution. Then, we show how these ideas can be directly extended to the very general L-spline smoothing.},
	number = {2},
	urldate = {2015-04-08},
	journal = {Bernoulli},
	author = {Abramovich, Felix and Grinshtein, Vadim},
	month = apr,
	year = {1999},
	mrnumber = {MR1681703},
	keywords = {Green's function, L-smoothing spline},
	pages = {359--379},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AKFMK999/1173147911.html:text/html}
}

@book{claeskens_model_2008,
	address = {Cambridge ; New York},
	edition = {1 edition},
	title = {Model {Selection} and {Model} {Averaging}},
	isbn = {9780521852258},
	abstract = {Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled with discussions of frequent and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R-code.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Claeskens, Gerda and Hjort, Nils Lid},
	month = jul,
	year = {2008}
}

@book{efron_introduction_1994,
	address = {New York},
	edition = {Softcover reprint of the original 1st ed. 1993 edition},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {9780412042317},
	abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Efron, Bradley and Tibshirani, R. J.},
	month = may,
	year = {1994}
}

@article{foster_variable_2004,
	title = {Variable {Selection} in {Data} {Mining}},
	volume = {99},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214504000000287},
	doi = {10.1198/016214504000000287},
	abstract = {We predict the onset of personal bankruptcy using least squares regression. Although well publicized, only 2,244 bankruptcies occur in our dataset of 2.9 million months of credit-card activity. We use stepwise selection to find predictors of these from a mix of payment history, debt load, demographics, and their interactions. This combination of rare responses and over 67,000 possible predictors leads to a challenging modeling question: How does one separate coincidental from useful predictors? We show that three modifications turn stepwise regression into an effective methodology for predicting bankruptcy. Our version of stepwise regression (1) organizes calculations to accommodate interactions, (2) exploits modern decision theoretic criteria to choose predictors, and (3) conservatively estimates p-values to handle sparse data and a binary response. Omitting any one of these leads to poor performance. A final step in our procedure calibrates regression predictions. With these modifications, stepwise regression predicts bankruptcy as well as, if not better than, recently developed data-mining tools. When sorted, the largest 14,000 resulting predictions hold 1,000 of the 1,800 bankruptcies hidden in a validation sample of 2.3 million observations. If the cost of missing a bankruptcy is 200 times that of a false positive, our predictions incur less than 2/3 of the costs of classification errors produced by the tree-based classifier C4.5.},
	number = {466},
	urldate = {2015-04-10},
	journal = {Journal of the American Statistical Association},
	author = {Foster, Dean P. and Stine, Robert A.},
	month = jun,
	year = {2004},
	pages = {303--313},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/DQGWU83P/Foster and Stine - 2004 - Variable Selection in Data Mining.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/S4I7VM9V/016214504000000287.html:text/html}
}

@inproceedings{gashler_decision_2008,
	title = {Decision {Tree} {Ensemble}: {Small} {Heterogeneous} {Is} {Better} {Than} {Large} {Homogeneous}},
	shorttitle = {Decision {Tree} {Ensemble}},
	doi = {10.1109/ICMLA.2008.154},
	abstract = {Using decision trees that split on randomly selected attributes is one way to increase the diversity within an ensemble of decision trees. Another approach increases diversity by combining multiple tree algorithms. The random forest approach has become popular because it is simple and yields good results with common datasets. We present a technique that combines heterogeneous tree algorithms and contrast it with homogeneous forest algorithms. Our results indicate that random forests do poorly when faced with irrelevant attributes, while our heterogeneous technique handles them robustly. Further, we show that large ensembles of random trees are more susceptible to diminishing returns than our technique. We are able to obtain better results across a large number of common datasets with a significantly smaller ensemble.},
	booktitle = {Seventh {International} {Conference} on {Machine} {Learning} and {Applications}, 2008. {ICMLA} '08},
	author = {Gashler, M. and Giraud-Carrier, C. and Martinez, T.},
	month = dec,
	year = {2008},
	keywords = {accuracy, Algorithm design and analysis, Application software, Bagging, Computer science, decision trees, Diversity reception, machine learning, robustness, Training data},
	pages = {900--905},
	file = {IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/ZADWDQMT/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/A2KMI5RD/Gashler et al. - 2008 - Decision Tree Ensemble Small Heterogeneous Is Bet.pdf:application/pdf}
}

@article{schapire_strength_1990,
	title = {The strength of weak learnability},
	volume = {5},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/BF00116037},
	doi = {10.1007/BF00116037},
	abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error ο.},
	language = {en},
	number = {2},
	urldate = {2015-04-11},
	journal = {Machine Learning},
	author = {Schapire, Robert E.},
	month = jun,
	year = {1990},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Science, general, learnability theory, learning from examples, machine learning, PAC learning, polynomial-time identification},
	pages = {197--227},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/5TFTM7C7/Schapire - 1990 - The strength of weak learnability.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AEZW5JHW/BF00116037.html:text/html}
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2015-04-11},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {ScienceDirect Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/8VN4SKVT/Freund and Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Lea.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/S2CSABX9/S002200009791504X.html:text/html}
}

@article{friedman_additive_2000,
	title = {Additive logistic regression: a statistical view of boosting ({With} discussion and a rejoinder by the authors)},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Additive logistic regression},
	url = {http://projecteuclid.org/euclid.aos/1016218223},
	doi = {10.1214/aos/1016218223},
	abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
	language = {EN},
	number = {2},
	urldate = {2015-04-11},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790002},
	zmnumber = {01828945},
	keywords = {classification, machine learning, nonparametric estimation, stagewise fitting, tree},
	pages = {337--407},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/TEW4QB6M/1016218223.html:text/html}
}

@inproceedings{agraval_fast_1994,
	title = {‘{Fast} {Algorithms} for {Mining} {Association} {Rules} in {Large} {Data} {Bases}},
	booktitle = {20th {Inter}-national {Conference} on {Very} {Large} {Databases}, {Santiagom}},
	author = {Agraval, R. and Srikant, Ramakrishnan},
	year = {1994}
}

@book{wasserman_all_2004,
	address = {New York},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {0387402721  9780387402727},
	shorttitle = {All of statistics},
	abstract = {"This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining, and machine learning." "This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping, and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate levels."--BOOK JACKET.},
	language = {English},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	file = {All Of Statistics [Wasserman].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/R7TU8M27/All Of Statistics [Wasserman].pdf:application/pdf}
}

@book{wahba_spline_1990,
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {9781611970128},
	abstract = {This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. The estimate is a polynomial smoothing spline. By placing this smoothing problem in the setting of reproducing kernel Hilbert spaces, a theory is developed which includes univariate smoothing splines, thin plate splines in d dimensions, splines on the sphere, additive splines, and interaction splines in a single framework. A straightforward generalization allows the theory to encompass the very important area of (Tikhonov) regularization methods for ill-posed inverse problems. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a wide variety of problems which fall within this framework. Methods for including side conditions and other prior information in solving ill-posed inverse problems are included. Data which involves samples of random variables with Gaussian, Poisson, binomial, and other distributions are treated in a unified optimization context. Experimental design questions, i.e., which functionals should be observed, are studied in a general context. Extensions to distributed parameter system identification problems are made by considering implicitly defined functionals.},
	language = {en},
	publisher = {SIAM},
	author = {Wahba, Grace},
	year = {1990}
}

@article{hyvarinen_independent_2000,
	title = {Independent component analysis: algorithms and applications},
	volume = {13},
	issn = {0893-6080},
	shorttitle = {Independent component analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608000000265},
	doi = {10.1016/S0893-6080(00)00026-5},
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	number = {4–5},
	urldate = {2015-04-22},
	journal = {Neural Networks},
	author = {Hyvärinen, A. and Oja, E.},
	month = jun,
	year = {2000},
	keywords = {Blind signal separation, Factor analysis, Independent component analysis, Projection pursuit, Representation, Source separation},
	pages = {411--430},
	file = {ScienceDirect Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/F2W3Z8N2/Hyvärinen and Oja - 2000 - Independent component analysis algorithms and app.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PZFNWT6F/S0893608000000265.html:text/html}
}

@book{kalman_contributions_1960,
	title = {Contributions to the {Theory} of {Optimal} {Control}},
	abstract = {This paper was in fact the first to introduce the RDE as an algorithm for computing the state feedback gain of the optimal controller for a general linear system with a quadratic performance criterion. RDE had emerged earlier in the study of the second variations in the calculus of variations, but its use in general linear systems, where the optimal trajectory needs to be generated by a control input, was new. The analysis throughout the paper concentrates on timevarying systems, and uses the Hamilton-Jacobi theory to arrive at RDE and to deduce optimality of the LQ control gain. We now know, however, that an alternative way to prove optimality in least squares is by showing how RDE allows one to "complete the square" (see, e.g., [5], [18]).},
	author = {Kalman, R. E.},
	year = {1960},
	file = {Citeseer - Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/GIJ8U5MC/Kalman - 1960 - Contributions to the Theory of Optimal Control.pdf:application/pdf;Citeseer - Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PTDAZEF7/summary.html:text/html}
}

@book{borg_modern_2005,
	address = {New York},
	edition = {2nd edition},
	title = {Modern {Multidimensional} {Scaling}: {Theory} and {Applications}},
	isbn = {9780387251509},
	shorttitle = {Modern {Multidimensional} {Scaling}},
	abstract = {The first edition was released in 1996 and has sold close to 2200 copies. Provides an up-to-date comprehensive treatment of MDS, a statistical technique used to analyze the structure of similarity or dissimilarity data in multidimensional space. The authors have added three chapters and exercise sets. The text is being moved from SSS to SSPP. The book is suitable for courses in statistics for the social or managerial sciences as well as for advanced courses on MDS. All the mathematics required for more advanced topics is developed systematically in the text.},
	language = {English},
	publisher = {Springer},
	author = {Borg, I. and Groenen, P. J. F.},
	month = aug,
	year = {2005}
}

@book{jolliffe_principal_2002,
	address = {New York},
	edition = {2nd edition},
	title = {Principal {Component} {Analysis}},
	isbn = {9780387954424},
	abstract = {The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition.},
	language = {English},
	publisher = {Springer},
	author = {Jolliffe, I. T.},
	month = oct,
	year = {2002}
}

@article{hotelling_analysis_1933,
	title = {Analysis of a complex of statistical variables into principal components},
	volume = {24},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1939-2176(Electronic);0022-0663(Print)},
	doi = {10.1037/h0071325},
	abstract = {The problem is stated in detail, a method of analysis is derived and its geometrical meaning shown, methods of solution are illustrated and certain derivative problems are discussed. (To be concluded in October issue.)},
	number = {6},
	journal = {Journal of Educational Psychology},
	author = {Hotelling, H.},
	year = {1933},
	keywords = {*Statistical Analysis, Statistical Variables},
	pages = {417--441},
	file = {APA Psycnet Fulltext PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/NMZE2KH5/Hotelling - 1933 - Analysis of a complex of statistical variables int.pdf:application/pdf}
}

@article{pearson_liii._1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	volume = {2},
	number = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {Pearson, Karl},
	year = {1901},
	pages = {559--572}
}

@incollection{johnson_extensions_1984,
	series = {Contemp. {Math}.},
	title = {Extensions of {Lipschitz} mappings into a {Hilbert} space},
	volume = {26},
	url = {http://www.ams.org/mathscinet-getitem?mr=737400},
	urldate = {2015-04-24},
	booktitle = {Conference in modern analysis and probability ({New} {Haven}, {Conn}., 1982)},
	publisher = {Amer. Math. Soc., Providence, RI},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	mrnumber = {737400},
	pages = {189--206},
	file = {MathSciNet Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/5XEK7WMN/mathscinet-getitem.html:text/html}
}

@inproceedings{tishby_information_1999,
	title = {The {Information} {Bottleneck} {Method}},
	abstract = {We dene the relevant information in a signal x 2 X as being the information  that this signal provides about another signal y 2 Y . Examples include the information  that face images provide about the names of the people portrayed, or  the information that speech sounds provide about the words spoken. Understanding  the signal x requires more than just predicting y, it also requires specifying  which features of X play a role in the prediction. We formalize the problem as  that of nding a short code for X that preserves the maximum information about  Y . That is, we squeeze the information that X provides about Y through a `bottleneck  ' formed by a limited set of codewords  {\textasciitilde} X. This constrained optimization  problem can be seen as a generalization of rate distortion theory in which the distortion  measure d(x; {\textasciitilde} x) emerges from the joint statistics of X and Y . The approach  yields an exact set of self-consistent equations for the coding rules X !  {\textasciitilde} X and  {\textasciitilde} X ! Y . Solutions to t...},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {1999},
	pages = {368--377},
	file = {Citeseer - Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/Q9PM2INZ/Tishby et al. - 1999 - The Information Bottleneck Method.pdf:application/pdf;Citeseer - Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/8AGXBBAP/summary.html:text/html}
}