
@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1023/A%3A1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2015-05-27},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, classification, Computing Methodologies, ensemble, Language Translation and Linguistics, regression, Simulation and Modeling},
	pages = {5--32},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/P5EA2P9T/Breiman - 2001 - Random Forests.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/I3M6FIU4/A1010933404324.html:text/html}
}

@book{vaart_asymptotic_1998,
	address = {Cambridge, UK ; New York, NY, USA},
	title = {Asymptotic {Statistics}},
	isbn = {9780521496032},
	abstract = {Here is a practical and mathematically rigorous introduction to the field of asymptotic statistics. In addition to most of the standard topics of an asymptotics course--likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures--the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, one of the book's unifying themes that mainly entails the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	month = oct,
	year = {1998},
	file = {Asymptotic Statistics [van der Vaart].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/URPF9ZVW/Asymptotic Statistics [van der Vaart].pdf:application/pdf}
}

@book{abramovich_statistical_2013,
	title = {Statistical theory: a concise introduction},
	isbn = {9781439851845  1439851840},
	shorttitle = {Statistical theory},
	abstract = {"Designed for a one-semester advanced undergraduate or graduate course, Statistical Theory: A Concise Introduction clearly explains the underlying ideas and principles of major statistical concepts, including parameter estimation, confidence intervals, hypothesis testing, asymptotic analysis, Bayesian inference, and elements of decision theory. It introduces these topics on a clear intuitive level using illustrative examples in addition to the formal definitions, theorems, and proofs. Based on the authors' lecture notes, this student-oriented, self-contained book maintains a proper balance between the clarity and rigor of exposition. In a few cases, the authors present a 'sketched' version of a proof, explaining its main ideas rather than giving detailed technical mathematical and probabilistic arguments. Chapters and sections marked by asterisks contain more advanced topics and may be omitted. A special chapter on linear models shows how the main theoretical concepts can be applied to the well-known and frequently used statistical tool of linear regression.Requiring no heavy calculus, simple questions throughout the text help students check their understanding of the material. Each chapter also includes a set of exercises that range in level of difficulty"-- "Preface This book is intended as a textbook for a one-term course in statistical theory for advanced undergraduates in statistics, mathematics or other related fields although at least parts of it may be useful for graduates as well. Although there exist many good books on the topic, having taught a one-term Statistical Theory course during the years we felt that it is somewhat hard to recommend a particular one as a proper textbook to undergraduate students in statistics. Some of the existing textbooks with a primary focus on rigorous formalism, in our view, do not explain sufficiently clearly the underlying ideas and principles of the main statistical concepts, and are more suitable for graduates. Some others are "all-inclusive" textbooks that include a variety of topics in statistics that make them "too heavy" for a one-term course in statistical theory. Our main motivation was to propose a more "student-oriented" self-contained textbook designed for a one-term course on statistical theory that would introduce basic statistical concepts first on a clear intuitive level with illustrative examples in addition to the (necessary!) formal definitions, theorems and proofs. It is based on our lecture notes. We tried to keep a proper balance between the clarity and rigorousness of exposition. In a few cases we preferred to present a "sketched" version of a proof explaining its main ideas or even to give it up at all rather then to follow detailed technical mathematical and probabilistic arguments. The interested reader can complete those proofs from other existing books on mathematical statistics (see the bibliography)"--},
	language = {English},
	author = {Abramovich, Felix and Ritov, Ya'acov},
	year = {2013}
}

@book{hastie_elements_2003,
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {0387952845},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics.

Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry.

The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.

FROM THE REVIEWS:

TECHNOMETRICS "[This] is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
	urldate = {2014-12-11},
	publisher = {Springer},
	author = {Hastie, T and Tibshirani, R and Friedman, JH},
	month = jul,
	year = {2003},
	keywords = {machine-learning, statistic}
}

@article{goldenberg_survey_2010,
	title = {A {Survey} of {Statistical} {Network} {Models}},
	volume = {2},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000005},
	doi = {10.1561/2200000005},
	abstract = {Networks are ubiquitous in science and have become a focal point for discussion in everyday life. Formal statistical models for the analysis of network data have emerged as a major topic of interest in diverse areas of study, and most of these involve a form of graphical representation. Probability models on graphs date back to 1959. Along with empirical studies in social psychology and sociology from the 1960s, these early works generated an active "network community" and a substantial literature in the 1970s. This effort moved into the statistical literature in the late 1970s and 1980s, and the past decade has seen a burgeoning network literature in statistical physics and computer science. The growth of the World Wide Web and the emergence of online "networking communities" such as Facebook, MySpace, and LinkedIn, and a host of more specialized professional network communities has intensified interest in the study of networks and network data. Our goal in this review is to provide the reader with an entry point to this burgeoning literature. We begin with an overview of the historical development of statistical network modeling and then we introduce a number of examples that have been studied in the network literature. Our subsequent discussion focuses on a number of prominent static and dynamic network models and their interconnections. We emphasize formal model descriptions, and pay special attention to the interpretation of parameters and their estimation. We end with a description of some open problems and challenges for machine learning and statistics.},
	number = {2},
	urldate = {2013-04-03},
	journal = {Found. Trends Mach. Learn.},
	author = {Goldenberg, A. and Zheng, A.X. and Fienberg, S.E. and Airoldi, E.M.},
	month = feb,
	year = {2010},
	pages = {129--233},
	file = {10.1.1.169.2632.pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/U5XIB8NC/10.1.1.169.2632.pdf:application/pdf}
}

@article{efron_improvements_1997,
	title = {Improvements on {Cross}-{Validation}: {The} .632+ {Bootstrap} {Method}},
	volume = {92},
	copyright = {Copyright © 1997 American Statistical Association},
	issn = {0162-1459},
	shorttitle = {Improvements on {Cross}-{Validation}},
	url = {http://www.jstor.org/stable/2965703},
	doi = {10.2307/2965703},
	abstract = {A training set of data has been used to construct a rule for predicting future responses. What is the error rate of this rule? This is an important question both for comparing models and for assessing a final selected model. The traditional answer to this question is given by cross-validation. The cross-validation estimate of prediction error is nearly unbiased but can be highly variable. Here we discuss bootstrap estimates of prediction error, which can be thought of as smoothed versions of cross-validation. We show that a particular bootstrap method, the .632+ rule, substantially outperforms cross-validation in a catalog of 24 simulation experiments. Besides providing point estimates, we also consider estimating the variability of an error rate estimate. All of the results here are nonparametric and apply to any possible prediction rule; however, we study only classification problems with 0-1 loss in detail. Our simulations include "smooth" prediction rules like Fisher's linear discriminant function and unsmooth ones like nearest neighbors.},
	number = {438},
	urldate = {2015-05-27},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley and Tibshirani, Robert},
	month = jun,
	year = {1997},
	pages = {548--560},
	file = {JSTOR Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/QR5QUGQ9/Efron and Tibshirani - 1997 - Improvements on Cross-Validation The .632+ Bootst.pdf:application/pdf}
}

@book{james_introduction_2013,
	address = {New York},
	edition = {1st ed. 2013. Corr. 4th printing 2014 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {9781461471370},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = aug,
	year = {2013}
}

@book{mccullagh_generalized_1989,
	address = {Boca Raton},
	edition = {2 edition},
	title = {Generalized {Linear} {Models}, {Second} {Edition}},
	isbn = {9780412317606},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {McCullagh, P. and Nelder, John A.},
	month = aug,
	year = {1989}
}

@book{vapnik_statistical_1998,
	address = {New York},
	edition = {1 edition},
	title = {Statistical {Learning} {Theory}},
	isbn = {9780471030034},
	abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Vapnik, Vladimir N.},
	month = sep,
	year = {1998}
}

@article{schmidhuber_deep_2015,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep {Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2015-04-08},
	journal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	month = jan,
	year = {2015},
	note = {arXiv: 1404.7828},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {85--117},
	file = {arXiv\:1404.7828 PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PSV772F4/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/RG8VU9J2/1404.html:text/html}
}

@book{shalev-shwartz_understanding_2014,
	address = {New York, NY},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {9781107057135},
	shorttitle = {Understanding {Machine} {Learning}},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	month = may,
	year = {2014}
}

@article{abramovich_derivation_1999,
	title = {Derivation of equivalent kernel for general spline smoothing: a systematic approach},
	volume = {5},
	issn = {1350-7265},
	shorttitle = {Derivation of equivalent kernel for general spline smoothing},
	url = {http://projecteuclid.org/euclid.bj/1173147911},
	abstract = {We consider first the spline smoothing nonparametric estimation with variable smoothing parameter and arbitrary design density function and show that the corresponding equivalent kernel can be approximated by the Green function of a certain linear differential operator. Furthermore, we propose to use the standard (in applied mathematics and engineering) method for asymptotic solution of linear differential equations, known as the Wentzel-Kramers-Brillouin method, for systematic derivation of an asymptotically equivalent kernel in this general case. The corresponding results for polynomial splines are a special case of the general solution. Then, we show how these ideas can be directly extended to the very general L-spline smoothing.},
	number = {2},
	urldate = {2015-04-08},
	journal = {Bernoulli},
	author = {Abramovich, Felix and Grinshtein, Vadim},
	month = apr,
	year = {1999},
	mrnumber = {MR1681703},
	keywords = {Green's function, L-smoothing spline},
	pages = {359--379},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AKFMK999/1173147911.html:text/html}
}

@book{claeskens_model_2008,
	address = {Cambridge ; New York},
	edition = {1 edition},
	title = {Model {Selection} and {Model} {Averaging}},
	isbn = {9780521852258},
	abstract = {Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled with discussions of frequent and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R-code.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Claeskens, Gerda and Hjort, Nils Lid},
	month = jul,
	year = {2008}
}

@book{efron_introduction_1994,
	address = {New York},
	edition = {Softcover reprint of the original 1st ed. 1993 edition},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {9780412042317},
	abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Efron, Bradley and Tibshirani, R. J.},
	month = may,
	year = {1994}
}

@article{foster_variable_2004,
	title = {Variable {Selection} in {Data} {Mining}},
	volume = {99},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214504000000287},
	doi = {10.1198/016214504000000287},
	abstract = {We predict the onset of personal bankruptcy using least squares regression. Although well publicized, only 2,244 bankruptcies occur in our dataset of 2.9 million months of credit-card activity. We use stepwise selection to find predictors of these from a mix of payment history, debt load, demographics, and their interactions. This combination of rare responses and over 67,000 possible predictors leads to a challenging modeling question: How does one separate coincidental from useful predictors? We show that three modifications turn stepwise regression into an effective methodology for predicting bankruptcy. Our version of stepwise regression (1) organizes calculations to accommodate interactions, (2) exploits modern decision theoretic criteria to choose predictors, and (3) conservatively estimates p-values to handle sparse data and a binary response. Omitting any one of these leads to poor performance. A final step in our procedure calibrates regression predictions. With these modifications, stepwise regression predicts bankruptcy as well as, if not better than, recently developed data-mining tools. When sorted, the largest 14,000 resulting predictions hold 1,000 of the 1,800 bankruptcies hidden in a validation sample of 2.3 million observations. If the cost of missing a bankruptcy is 200 times that of a false positive, our predictions incur less than 2/3 of the costs of classification errors produced by the tree-based classifier C4.5.},
	number = {466},
	urldate = {2015-04-10},
	journal = {Journal of the American Statistical Association},
	author = {Foster, Dean P. and Stine, Robert A.},
	month = jun,
	year = {2004},
	pages = {303--313},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/DQGWU83P/Foster and Stine - 2004 - Variable Selection in Data Mining.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/S4I7VM9V/016214504000000287.html:text/html}
}

@inproceedings{gashler_decision_2008,
	title = {Decision {Tree} {Ensemble}: {Small} {Heterogeneous} {Is} {Better} {Than} {Large} {Homogeneous}},
	shorttitle = {Decision {Tree} {Ensemble}},
	doi = {10.1109/ICMLA.2008.154},
	abstract = {Using decision trees that split on randomly selected attributes is one way to increase the diversity within an ensemble of decision trees. Another approach increases diversity by combining multiple tree algorithms. The random forest approach has become popular because it is simple and yields good results with common datasets. We present a technique that combines heterogeneous tree algorithms and contrast it with homogeneous forest algorithms. Our results indicate that random forests do poorly when faced with irrelevant attributes, while our heterogeneous technique handles them robustly. Further, we show that large ensembles of random trees are more susceptible to diminishing returns than our technique. We are able to obtain better results across a large number of common datasets with a significantly smaller ensemble.},
	booktitle = {Seventh {International} {Conference} on {Machine} {Learning} and {Applications}, 2008. {ICMLA} '08},
	author = {Gashler, M. and Giraud-Carrier, C. and Martinez, T.},
	month = dec,
	year = {2008},
	keywords = {accuracy, Algorithm design and analysis, Application software, Bagging, Computer science, decision trees, Diversity reception, machine learning, robustness, Training data},
	pages = {900--905},
	file = {IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/ZADWDQMT/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/A2KMI5RD/Gashler et al. - 2008 - Decision Tree Ensemble Small Heterogeneous Is Bet.pdf:application/pdf}
}

@article{balakrishnan_statistical_2014,
	title = {Statistical guarantees for the {EM} algorithm: {From} population to sample-based analysis},
	shorttitle = {Statistical guarantees for the {EM} algorithm},
	url = {http://arxiv.org/abs/1408.2156},
	abstract = {We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM. Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples. First, we characterize the domain of attraction of any global maximizer of the population likelihood. This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent. Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples. We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random. In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE. We provide simulations to confirm this theoretically predicted behavior.},
	urldate = {2014-08-28},
	journal = {arXiv:1408.2156 [cs, math, stat]},
	author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.2156},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv\:1408.2156 PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/EN966EAF/Balakrishnan et al. - 2014 - Statistical guarantees for the EM algorithm From .pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/QWIN42DM/1408.html:text/html}
}

@article{schapire_strength_1990,
	title = {The strength of weak learnability},
	volume = {5},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/BF00116037},
	doi = {10.1007/BF00116037},
	abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error ο.},
	language = {en},
	number = {2},
	urldate = {2015-04-11},
	journal = {Machine Learning},
	author = {Schapire, Robert E.},
	month = jun,
	year = {1990},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Science, general, learnability theory, learning from examples, machine learning, PAC learning, polynomial-time identification},
	pages = {197--227},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/5TFTM7C7/Schapire - 1990 - The strength of weak learnability.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AEZW5JHW/BF00116037.html:text/html}
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2015-04-11},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {ScienceDirect Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/8VN4SKVT/Freund and Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Lea.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/S2CSABX9/S002200009791504X.html:text/html}
}

@article{friedman_additive_2000,
	title = {Additive logistic regression: a statistical view of boosting ({With} discussion and a rejoinder by the authors)},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Additive logistic regression},
	url = {http://projecteuclid.org/euclid.aos/1016218223},
	doi = {10.1214/aos/1016218223},
	abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
	language = {EN},
	number = {2},
	urldate = {2015-04-11},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790002},
	zmnumber = {01828945},
	keywords = {classification, machine learning, nonparametric estimation, stagewise fitting, tree},
	pages = {337--407},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/TEW4QB6M/1016218223.html:text/html}
}

@inproceedings{agraval_fast_1994,
	title = {‘{Fast} {Algorithms} for {Mining} {Association} {Rules} in {Large} {Data} {Bases}},
	booktitle = {20th {Inter}-national {Conference} on {Very} {Large} {Databases}, {Santiagom}},
	author = {Agraval, R. and Srikant, Ramakrishnan},
	year = {1994}
}

@book{wasserman_all_2004,
	address = {New York},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {0387402721  9780387402727},
	shorttitle = {All of statistics},
	abstract = {"This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining, and machine learning." "This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping, and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate levels."--BOOK JACKET.},
	language = {English},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	file = {All Of Statistics [Wasserman].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/R7TU8M27/All Of Statistics [Wasserman].pdf:application/pdf}
}

@article{fernandez-delgado_we_2014,
	title = {Do {We} {Need} {Hundreds} of {Classifiers} to {Solve} {Real} {World} {Classification} {Problems}?},
	volume = {15},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2697065},
	abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1\% of the maximum accuracy overcoming 90\% in the 84.3\% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3\% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
	number = {1},
	urldate = {2015-05-27},
	journal = {J. Mach. Learn. Res.},
	author = {Fernández-Delgado, Manuel and Cernadas, Eva and Barro, Senén and Amorim, Dinani},
	month = jan,
	year = {2014},
	keywords = {Bayesian classifiers, classification, decision trees, discriminant analysis, ensembles, generalized linear models, logistic and multinomial regression, multiple adaptive regression splines, nearest-neighbors, neural networks, partial least squares and principal component regression, random forest, rule-based classifiers, support vector machine, UCI data base},
	pages = {3133--3181},
	file = {ACM Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AN2BW3VN/Fernández-Delgado et al. - 2014 - Do We Need Hundreds of Classifiers to Solve Real W.pdf:application/pdf}
}

@article{rodriguez_rotation_2006,
	title = {Rotation {Forest}: {A} {New} {Classifier} {Ensemble} {Method}},
	volume = {28},
	issn = {0162-8828},
	shorttitle = {Rotation {Forest}},
	doi = {10.1109/TPAMI.2006.211},
	abstract = {We propose a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and principal component analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name "forest". Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Using WEKA, we examined the rotation forest ensemble on a random selection of 33 benchmark data sets from the UCI repository and compared it with bagging, AdaBoost, and random forest. The results were favorable to rotation forest and prompted an investigation into diversity-accuracy landscape of the ensemble models. Diversity-error diagrams revealed that rotation forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and random forest, and more diverse than these in bagging, sometimes more accurate as well},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rodriguez, J.J. and Kuncheva, L.I. and Alonso, C.J.},
	month = oct,
	year = {2006},
	keywords = {AdaBoost, Algorithms, Artificial Intelligence, Bagging, Classification tree analysis, classifier ensemble method, Classifier ensembles, Cluster Analysis, Computer Simulation, Computer Society, decision trees, diversity-accuracy landscape, feature extraction, Information Storage and Retrieval, kappa-error diagrams., machine learning, Models, Statistical, Numerical Analysis, Computer-Assisted, pattern classification, pattern recognition, Pattern Recognition, Automated, pca, principal component analysis, random forest, Reproducibility of Results, rotation forest, Sensitivity and Specificity, Training data, Voting},
	pages = {1619--1630},
	file = {IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/5UMGXZ56/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/RQK2TJN8/Rodriguez et al. - 2006 - Rotation Forest A New Classifier Ensemble Method.pdf:application/pdf}
}

@book{wahba_spline_1990,
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {9781611970128},
	abstract = {This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. The estimate is a polynomial smoothing spline. By placing this smoothing problem in the setting of reproducing kernel Hilbert spaces, a theory is developed which includes univariate smoothing splines, thin plate splines in d dimensions, splines on the sphere, additive splines, and interaction splines in a single framework. A straightforward generalization allows the theory to encompass the very important area of (Tikhonov) regularization methods for ill-posed inverse problems. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a wide variety of problems which fall within this framework. Methods for including side conditions and other prior information in solving ill-posed inverse problems are included. Data which involves samples of random variables with Gaussian, Poisson, binomial, and other distributions are treated in a unified optimization context. Experimental design questions, i.e., which functionals should be observed, are studied in a general context. Extensions to distributed parameter system identification problems are made by considering implicitly defined functionals.},
	language = {en},
	publisher = {SIAM},
	author = {Wahba, Grace},
	year = {1990}
}

@article{hyvarinen_independent_2000,
	title = {Independent component analysis: algorithms and applications},
	volume = {13},
	issn = {0893-6080},
	shorttitle = {Independent component analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608000000265},
	doi = {10.1016/S0893-6080(00)00026-5},
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	number = {4–5},
	urldate = {2015-04-22},
	journal = {Neural Networks},
	author = {Hyvärinen, A. and Oja, E.},
	month = jun,
	year = {2000},
	keywords = {Blind signal separation, Factor analysis, Independent component analysis, Projection pursuit, Representation, Source separation},
	pages = {411--430},
	file = {ScienceDirect Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/F2W3Z8N2/Hyvärinen and Oja - 2000 - Independent component analysis algorithms and app.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PZFNWT6F/S0893608000000265.html:text/html}
}

@book{kalman_contributions_1960,
	title = {Contributions to the {Theory} of {Optimal} {Control}},
	abstract = {This paper was in fact the first to introduce the RDE as an algorithm for computing the state feedback gain of the optimal controller for a general linear system with a quadratic performance criterion. RDE had emerged earlier in the study of the second variations in the calculus of variations, but its use in general linear systems, where the optimal trajectory needs to be generated by a control input, was new. The analysis throughout the paper concentrates on timevarying systems, and uses the Hamilton-Jacobi theory to arrive at RDE and to deduce optimality of the LQ control gain. We now know, however, that an alternative way to prove optimality in least squares is by showing how RDE allows one to "complete the square" (see, e.g., [5], [18]).},
	author = {Kalman, R. E.},
	year = {1960},
	file = {Citeseer - Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/GIJ8U5MC/Kalman - 1960 - Contributions to the Theory of Optimal Control.pdf:application/pdf;Citeseer - Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PTDAZEF7/summary.html:text/html}
}

@book{borg_modern_2005,
	address = {New York},
	edition = {2nd edition},
	title = {Modern {Multidimensional} {Scaling}: {Theory} and {Applications}},
	isbn = {9780387251509},
	shorttitle = {Modern {Multidimensional} {Scaling}},
	abstract = {The first edition was released in 1996 and has sold close to 2200 copies. Provides an up-to-date comprehensive treatment of MDS, a statistical technique used to analyze the structure of similarity or dissimilarity data in multidimensional space. The authors have added three chapters and exercise sets. The text is being moved from SSS to SSPP. The book is suitable for courses in statistics for the social or managerial sciences as well as for advanced courses on MDS. All the mathematics required for more advanced topics is developed systematically in the text.},
	language = {English},
	publisher = {Springer},
	author = {Borg, I. and Groenen, P. J. F.},
	month = aug,
	year = {2005}
}

@book{jolliffe_principal_2002,
	address = {New York},
	edition = {2nd edition},
	title = {Principal {Component} {Analysis}},
	isbn = {9780387954424},
	abstract = {The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition.},
	language = {English},
	publisher = {Springer},
	author = {Jolliffe, I. T.},
	month = oct,
	year = {2002}
}

@article{hotelling_analysis_1933,
	title = {Analysis of a complex of statistical variables into principal components},
	volume = {24},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1939-2176(Electronic);0022-0663(Print)},
	doi = {10.1037/h0071325},
	abstract = {The problem is stated in detail, a method of analysis is derived and its geometrical meaning shown, methods of solution are illustrated and certain derivative problems are discussed. (To be concluded in October issue.)},
	number = {6},
	journal = {Journal of Educational Psychology},
	author = {Hotelling, H.},
	year = {1933},
	keywords = {*Statistical Analysis, Statistical Variables},
	pages = {417--441},
	file = {APA Psycnet Fulltext PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/NMZE2KH5/Hotelling - 1933 - Analysis of a complex of statistical variables int.pdf:application/pdf}
}

@article{olding_inference_2014,
	title = {Inference for graphs and networks: {Extending} classical tools to modern data},
	shorttitle = {Inference for graphs and networks},
	url = {http://arxiv.org/abs/0906.4980},
	doi = {10.1142/9781783263752_0001},
	abstract = {Graphs and networks provide a canonical representation of relational data, with massive network data sets becoming increasingly prevalent across a variety of scientific fields. Although tools from mathematics and computer science have been eagerly adopted by practitioners in the service of network inference, they do not yet comprise a unified and coherent framework for the statistical analysis of large-scale network data. This paper serves as both an introduction to the topic and a first step toward formal inference procedures. We develop and illustrate our arguments using the example of hypothesis testing for network structure. We invoke a generalized likelihood ratio framework and use it to highlight the growing number of topics in this area that require strong contributions from statistical science. We frame our discussion in the context of previous work from across a variety of disciplines, and conclude by outlining fundamental statistical challenges whose solutions will in turn serve to advance the science of network inference.},
	urldate = {2015-04-25},
	journal = {arXiv:0906.4980 [stat]},
	author = {Olding, Benjamin P. and Wolfe, Patrick J.},
	month = apr,
	year = {2014},
	note = {arXiv: 0906.4980},
	keywords = {Statistics - Applications, Statistics - Methodology},
	pages = {1--31},
	file = {arXiv\:0906.4980 PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/WR9Z8E2T/Olding and Wolfe - 2014 - Inference for graphs and networks Extending class.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/6BR2VX3W/0906.html:text/html}
}

@article{pearson_liii._1901,
	title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
	volume = {2},
	number = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {Pearson, Karl},
	year = {1901},
	pages = {559--572}
}

@incollection{johnson_extensions_1984,
	series = {Contemp. {Math}.},
	title = {Extensions of {Lipschitz} mappings into a {Hilbert} space},
	volume = {26},
	url = {http://www.ams.org/mathscinet-getitem?mr=737400},
	urldate = {2015-04-24},
	booktitle = {Conference in modern analysis and probability ({New} {Haven}, {Conn}., 1982)},
	publisher = {Amer. Math. Soc., Providence, RI},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	mrnumber = {737400},
	pages = {189--206},
	file = {MathSciNet Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/5XEK7WMN/mathscinet-getitem.html:text/html}
}

@inproceedings{tishby_information_1999,
	title = {The {Information} {Bottleneck} {Method}},
	abstract = {We dene the relevant information in a signal x 2 X as being the information  that this signal provides about another signal y 2 Y . Examples include the information  that face images provide about the names of the people portrayed, or  the information that speech sounds provide about the words spoken. Understanding  the signal x requires more than just predicting y, it also requires specifying  which features of X play a role in the prediction. We formalize the problem as  that of nding a short code for X that preserves the maximum information about  Y . That is, we squeeze the information that X provides about Y through a `bottleneck  ' formed by a limited set of codewords  {\textasciitilde} X. This constrained optimization  problem can be seen as a generalization of rate distortion theory in which the distortion  measure d(x; {\textasciitilde} x) emerges from the joint statistics of X and Y . The approach  yields an exact set of self-consistent equations for the coding rules X !  {\textasciitilde} X and  {\textasciitilde} X ! Y . Solutions to t...},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {1999},
	pages = {368--377},
	file = {Citeseer - Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/Q9PM2INZ/Tishby et al. - 1999 - The Information Bottleneck Method.pdf:application/pdf;Citeseer - Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/8AGXBBAP/summary.html:text/html}
}

@book{meyer_matrix_2001,
	address = {Philadelphia},
	edition = {Har/Cdr edition},
	title = {Matrix {Analysis} and {Applied} {Linear} {Algebra} {Book} and {Solutions} {Manual}},
	isbn = {9780898714548},
	abstract = {This book avoids the traditional definition-theorem-proof format; instead a fresh approach introduces a variety of problems and examples all in a clear and informal style. The in-depth focus on applications separates this book from others, and helps students to see how linear algebra can be applied to real-life situations. Some of the more contemporary topics of applied linear algebra are included here which are not normally found in undergraduate textbooks. Theoretical developments are always accompanied with detailed examples, and each section ends with a number of exercises from which students can gain further insight. Moreover, the inclusion of historical information provides personal insights into the mathematicians who developed this subject. The textbook contains numerous examples and exercises, historical notes, and comments on numerical performance and the possible pitfalls of algorithms. Solutions to all of the exercises are provided, as well as a CD-ROM containing a searchable copy of the textbook.},
	language = {English},
	publisher = {SIAM: Society for Industrial and Applied Mathematics},
	author = {Meyer, Carl D.},
	month = feb,
	year = {2001}
}

@article{koren_matrix_2009,
	title = {Matrix {Factorization} {Techniques} for {Recommender} {Systems}},
	volume = {42},
	issn = {0018-9162},
	doi = {10.1109/MC.2009.263},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
	number = {8},
	journal = {Computer},
	author = {Koren, Y. and Bell, R. and Volinsky, C.},
	month = aug,
	year = {2009},
	keywords = {Bioinformatics, Collaboration, Computational intelligence, Filtering, genomics, information filtering, Matrix decomposition, Matrix factorization, matrix factorization technique, Motion pictures, Nearest neighbor searches, nearest neighbor technique, Netflix Prize, Netflix Prize competition, Predictive models, product recommendation system, recommender system, recommender systems, retail data processing, Sea measurements},
	pages = {30--37},
	file = {ieeecomputer.pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/TNPQMGIH/ieeecomputer.pdf:application/pdf;IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/HHZFRH6F/abs_all.html:text/html}
}

@article{friedman_projection_1974,
	title = {A {Projection} {Pursuit} {Algorithm} for {Exploratory} {Data} {Analysis}},
	volume = {C-23},
	issn = {0018-9340},
	doi = {10.1109/T-C.1974.224051},
	abstract = {An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.},
	number = {9},
	journal = {IEEE Transactions on Computers},
	author = {Friedman, J.H. and Tukey, J.W.},
	month = sep,
	year = {1974},
	keywords = {Algorithm design and analysis, Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric pattern recognition, statistics., Data analysis, Humans, Inspection, Iterative algorithms, Multidimensional systems, Pattern analysis, pattern recognition, Pursuit algorithms, Statistical analysis},
	pages = {881--890},
	file = {IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/ZF8KZ3FT/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/7C8Z3BIR/Friedman and Tukey - 1974 - A Projection Pursuit Algorithm for Exploratory Dat.pdf:application/pdf}
}

@article{su_survey_2009,
	title = {A {Survey} of {Collaborative} {Filtering} {Techniques}},
	volume = {2009},
	issn = {1687-7470},
	url = {http://www.hindawi.com/journals/aai/2009/421425/abs/},
	doi = {10.1155/2009/421425},
	abstract = {As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.},
	language = {en},
	urldate = {2015-04-30},
	journal = {Advances in Artificial Intelligence},
	author = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
	month = oct,
	year = {2009},
	pages = {e421425},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/UXAFHFBN/Su and Khoshgoftaar - 2009 - A Survey of Collaborative Filtering Techniques.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/BWM7BBA4/421425.html:application/xhtml+xml}
}

@article{braida_transforming_2015,
	title = {Transforming collaborative filtering into supervised learning},
	volume = {42},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S095741741500038X},
	doi = {10.1016/j.eswa.2015.01.023},
	abstract = {Collaborative Filtering (CF) is a well-known approach for Recommender Systems (RS). This approach extrapolates rating predictions from ratings given by user on items, which are represented by a user-item matrix filled with a rating r i , j given by an user i on an item j. Therefore, CF has been confined to this data structure relying mostly on adaptations of supervised learning methods to deal with rating predictions and matrix decomposition schemes to complete unfilled positions of the rating matrix. Although there have been proposals to apply Machine Learning (ML) to RS, these works had to transform the rating matrix into the typical Supervised Learning (SL) data set, i.e., a set of pairwise tuples ( x , y ) , where y is the correspondent class (the rating) of the instance x ∈ R k . So far, the proposed transformations were thoroughly crafted using the domain information. However, in many applications this kind of information can be incomplete, uncertain or stated in ways that are not machine-readable. Even when it is available, its usage can be very complex requiring specialists to craft the transformation. In this context, this work proposes a domain-independent transformation from the rating matrix representation to a supervised learning dataset that enables SL methods to be fully explored in RS. In addition, our transformation is said to be straightforward, in the sense that, it is an automatic process that any lay person can perform requiring no domain specialist. Our experiments have proven that our transformation, combined with SL methods, have greatly outperformed classical CF methods.},
	number = {10},
	urldate = {2015-04-30},
	journal = {Expert Systems with Applications},
	author = {Braida, Filipe and Mello, Carlos E. and Pasinato, Marden B. and Zimbrão, Geraldo},
	month = jun,
	year = {2015},
	keywords = {Collaborative filtering, Dimensionality reduction, recommender system, Supervised learning},
	pages = {4733--4742},
	file = {ScienceDirect Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/Q5EVNVZ2/Braida et al. - 2015 - Transforming collaborative filtering into supervis.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/XVDZEXSV/S095741741500038X.html:text/html}
}

@article{goldberg_using_1992,
	title = {Using {Collaborative} {Filtering} to {Weave} an {Information} {Tapestry}},
	volume = {35},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/138859.138867},
	doi = {10.1145/138859.138867},
	number = {12},
	urldate = {2015-04-30},
	journal = {Commun. ACM},
	author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
	month = dec,
	year = {1992},
	keywords = {information filtering, tapestry},
	pages = {61--70},
	file = {ACM Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/377VEEVZ/Goldberg et al. - 1992 - Using Collaborative Filtering to Weave an Informat.pdf:application/pdf}
}

@book{anderson_introduction_2003,
	address = {Hoboken, NJ},
	edition = {3 edition},
	title = {An {Introduction} to {Multivariate} {Statistical} {Analysis}},
	isbn = {9780471360919},
	abstract = {Perfected over three editions and more than forty years, this field- and classroom-tested reference: * Uses the method of maximum likelihood to a large extent to ensure reasonable, and in some cases optimal procedures. * Treats all the basic and important topics in multivariate statistics. * Adds two new chapters, along with a number of new sections. * Provides the most methodical, up-to-date information on MV statistics available.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Anderson, T. W.},
	month = jul,
	year = {2003}
}

@article{friedman_exploratory_1987,
	title = {Exploratory {Projection} {Pursuit}},
	volume = {82},
	copyright = {Copyright © 1987 American Statistical Association},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289161},
	doi = {10.2307/2289161},
	abstract = {A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary nonlinear effects are impossible to parameterize in full generality, they are easily recognized when presented in a low-dimensional visual representation of the data density. Projection pursuit assigns a numerical index to every projection that is a functional of the projected data density. The intent of this index is to capture the degree of nonlinear structuring present in the projected distribution. The pursuit consists of maximizing this index with respect to the parameters defining the projection. Since it is unlikely that there is only one interesting view of a multivariate data set, this procedure is iterated to find further revealing projections. After each maximizing projection has been found, a transformation is applied to the data that removes the structure present in the solution projection while preserving the multivariate structure that is not captured by it. The projection pursuit algorithm is then applied to these transformed data to find additional views that may yield further insight. This projection pursuit algorithm has potential advantages over other dimensionality reduction methods that are commonly used for data exploration. It focuses directly on the "interestingness" of a projection rather than indirectly through the interpoint distances. This allows it to be unaffected by the scale and (linear) correlational structure of the data, helping it to overcome the "curse of dimensionality" that tends to plague methods based on multidimensional scaling, parametric mapping, cluster analysis, and principal components.},
	number = {397},
	urldate = {2015-05-07},
	journal = {Journal of the American Statistical Association},
	author = {Friedman, Jerome H.},
	month = mar,
	year = {1987},
	pages = {249--266},
	file = {JSTOR Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/N65RMHK9/Friedman - 1987 - Exploratory Projection Pursuit.pdf:application/pdf}
}

@article{diaconis_asymptotics_1984,
	title = {Asymptotics of {Graphical} {Projection} {Pursuit}},
	volume = {12},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1176346703},
	doi = {10.1214/aos/1176346703},
	abstract = {Mathematical tools are developed for describing low-dimensional projections of high-dimensional data. Theorems are given to show that under suitable conditions, most projections are approximately Gaussian.},
	language = {EN},
	number = {3},
	urldate = {2015-05-07},
	journal = {The Annals of Statistics},
	author = {Diaconis, Persi and Freedman, David},
	month = sep,
	year = {1984},
	mrnumber = {MR751274},
	zmnumber = {0559.62002},
	keywords = {62-07, empirical characteristic function, graphical methods, Projection pursuit, Projections, random probabilities},
	pages = {793--815},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/T96JP69Z/1176346703.html:text/html}
}

@misc{lorica_lets_????,
	title = {Let’s build open source tensor libraries for data science - {O}'{Reilly} {Radar}},
	url = {http://radar.oreilly.com/2015/03/lets-build-open-source-tensor-libraries-for-data-science.html},
	abstract = {Data scientists frequently find themselves dealing with high-dimensional feature spaces. As an example, text mining usually involves vocabularies comprised of 10,000+ different words. Many analytic problems involve linear...},
	urldate = {2015-06-09},
	author = {Lorica, Ben},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/NHBA3N9A/lets-build-open-source-tensor-libraries-for-data-science.html:text/html}
}