
@book{vaart_asymptotic_1998,
	address = {Cambridge, UK ; New York, NY, USA},
	title = {Asymptotic {Statistics}},
	isbn = {9780521496032},
	abstract = {Here is a practical and mathematically rigorous introduction to the field of asymptotic statistics. In addition to most of the standard topics of an asymptotics course--likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures--the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, one of the book's unifying themes that mainly entails the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	month = oct,
	year = {1998},
	file = {Asymptotic Statistics [van der Vaart].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/URPF9ZVW/Asymptotic Statistics [van der Vaart].pdf:application/pdf}
}

@book{abramovich_statistical_2013,
	title = {Statistical theory: a concise introduction},
	isbn = {9781439851845  1439851840},
	shorttitle = {Statistical theory},
	abstract = {"Designed for a one-semester advanced undergraduate or graduate course, Statistical Theory: A Concise Introduction clearly explains the underlying ideas and principles of major statistical concepts, including parameter estimation, confidence intervals, hypothesis testing, asymptotic analysis, Bayesian inference, and elements of decision theory. It introduces these topics on a clear intuitive level using illustrative examples in addition to the formal definitions, theorems, and proofs. Based on the authors' lecture notes, this student-oriented, self-contained book maintains a proper balance between the clarity and rigor of exposition. In a few cases, the authors present a 'sketched' version of a proof, explaining its main ideas rather than giving detailed technical mathematical and probabilistic arguments. Chapters and sections marked by asterisks contain more advanced topics and may be omitted. A special chapter on linear models shows how the main theoretical concepts can be applied to the well-known and frequently used statistical tool of linear regression.Requiring no heavy calculus, simple questions throughout the text help students check their understanding of the material. Each chapter also includes a set of exercises that range in level of difficulty"-- "Preface This book is intended as a textbook for a one-term course in statistical theory for advanced undergraduates in statistics, mathematics or other related fields although at least parts of it may be useful for graduates as well. Although there exist many good books on the topic, having taught a one-term Statistical Theory course during the years we felt that it is somewhat hard to recommend a particular one as a proper textbook to undergraduate students in statistics. Some of the existing textbooks with a primary focus on rigorous formalism, in our view, do not explain sufficiently clearly the underlying ideas and principles of the main statistical concepts, and are more suitable for graduates. Some others are "all-inclusive" textbooks that include a variety of topics in statistics that make them "too heavy" for a one-term course in statistical theory. Our main motivation was to propose a more "student-oriented" self-contained textbook designed for a one-term course on statistical theory that would introduce basic statistical concepts first on a clear intuitive level with illustrative examples in addition to the (necessary!) formal definitions, theorems and proofs. It is based on our lecture notes. We tried to keep a proper balance between the clarity and rigorousness of exposition. In a few cases we preferred to present a "sketched" version of a proof explaining its main ideas or even to give it up at all rather then to follow detailed technical mathematical and probabilistic arguments. The interested reader can complete those proofs from other existing books on mathematical statistics (see the bibliography)"--},
	language = {English},
	author = {Abramovich, Felix and Ritov, Ya'acov},
	year = {2013}
}

@book{hastie_elements_2003,
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {0387952845},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics.

Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry.

The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.

FROM THE REVIEWS:

TECHNOMETRICS "[This] is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
	urldate = {2014-12-11},
	publisher = {Springer},
	author = {Hastie, T and Tibshirani, R and Friedman, JH},
	month = jul,
	year = {2003},
	keywords = {machine-learning, statistic}
}

@book{james_introduction_2013,
	address = {New York},
	edition = {1st ed. 2013. Corr. 4th printing 2014 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {9781461471370},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = aug,
	year = {2013}
}

@book{mccullagh_generalized_1989,
	address = {Boca Raton},
	edition = {2 edition},
	title = {Generalized {Linear} {Models}, {Second} {Edition}},
	isbn = {9780412317606},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {McCullagh, P. and Nelder, John A.},
	month = aug,
	year = {1989}
}

@book{vapnik_statistical_1998,
	address = {New York},
	edition = {1 edition},
	title = {Statistical {Learning} {Theory}},
	isbn = {9780471030034},
	abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Vapnik, Vladimir N.},
	month = sep,
	year = {1998}
}

@article{schmidhuber_deep_2015,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep {Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2015-04-08},
	journal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	month = jan,
	year = {2015},
	note = {arXiv: 1404.7828},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {85--117},
	file = {arXiv\:1404.7828 PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/PSV772F4/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/RG8VU9J2/1404.html:text/html}
}

@book{shalev-shwartz_understanding_2014,
	address = {New York, NY},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {9781107057135},
	shorttitle = {Understanding {Machine} {Learning}},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	month = may,
	year = {2014}
}

@article{abramovich_derivation_1999,
	title = {Derivation of equivalent kernel for general spline smoothing: a systematic approach},
	volume = {5},
	issn = {1350-7265},
	shorttitle = {Derivation of equivalent kernel for general spline smoothing},
	url = {http://projecteuclid.org/euclid.bj/1173147911},
	abstract = {We consider first the spline smoothing nonparametric estimation with variable smoothing parameter and arbitrary design density function and show that the corresponding equivalent kernel can be approximated by the Green function of a certain linear differential operator. Furthermore, we propose to use the standard (in applied mathematics and engineering) method for asymptotic solution of linear differential equations, known as the Wentzel-Kramers-Brillouin method, for systematic derivation of an asymptotically equivalent kernel in this general case. The corresponding results for polynomial splines are a special case of the general solution. Then, we show how these ideas can be directly extended to the very general L-spline smoothing.},
	number = {2},
	urldate = {2015-04-08},
	journal = {Bernoulli},
	author = {Abramovich, Felix and Grinshtein, Vadim},
	month = apr,
	year = {1999},
	mrnumber = {MR1681703},
	keywords = {Green's function, L-smoothing spline},
	pages = {359--379},
	file = {Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/AKFMK999/1173147911.html:text/html}
}

@book{claeskens_model_2008,
	address = {Cambridge ; New York},
	edition = {1 edition},
	title = {Model {Selection} and {Model} {Averaging}},
	isbn = {9780521852258},
	abstract = {Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled with discussions of frequent and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R-code.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Claeskens, Gerda and Hjort, Nils Lid},
	month = jul,
	year = {2008}
}

@book{efron_introduction_1994,
	address = {New York},
	edition = {Softcover reprint of the original 1st ed. 1993 edition},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {9780412042317},
	abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Efron, Bradley and Tibshirani, R. J.},
	month = may,
	year = {1994}
}

@article{foster_variable_2004,
	title = {Variable {Selection} in {Data} {Mining}},
	volume = {99},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214504000000287},
	doi = {10.1198/016214504000000287},
	abstract = {We predict the onset of personal bankruptcy using least squares regression. Although well publicized, only 2,244 bankruptcies occur in our dataset of 2.9 million months of credit-card activity. We use stepwise selection to find predictors of these from a mix of payment history, debt load, demographics, and their interactions. This combination of rare responses and over 67,000 possible predictors leads to a challenging modeling question: How does one separate coincidental from useful predictors? We show that three modifications turn stepwise regression into an effective methodology for predicting bankruptcy. Our version of stepwise regression (1) organizes calculations to accommodate interactions, (2) exploits modern decision theoretic criteria to choose predictors, and (3) conservatively estimates p-values to handle sparse data and a binary response. Omitting any one of these leads to poor performance. A final step in our procedure calibrates regression predictions. With these modifications, stepwise regression predicts bankruptcy as well as, if not better than, recently developed data-mining tools. When sorted, the largest 14,000 resulting predictions hold 1,000 of the 1,800 bankruptcies hidden in a validation sample of 2.3 million observations. If the cost of missing a bankruptcy is 200 times that of a false positive, our predictions incur less than 2/3 of the costs of classification errors produced by the tree-based classifier C4.5.},
	number = {466},
	urldate = {2015-04-10},
	journal = {Journal of the American Statistical Association},
	author = {Foster, Dean P. and Stine, Robert A.},
	month = jun,
	year = {2004},
	pages = {303--313},
	file = {Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/DQGWU83P/Foster and Stine - 2004 - Variable Selection in Data Mining.pdf:application/pdf;Snapshot:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/S4I7VM9V/016214504000000287.html:text/html}
}

@inproceedings{gashler_decision_2008,
	title = {Decision {Tree} {Ensemble}: {Small} {Heterogeneous} {Is} {Better} {Than} {Large} {Homogeneous}},
	shorttitle = {Decision {Tree} {Ensemble}},
	doi = {10.1109/ICMLA.2008.154},
	abstract = {Using decision trees that split on randomly selected attributes is one way to increase the diversity within an ensemble of decision trees. Another approach increases diversity by combining multiple tree algorithms. The random forest approach has become popular because it is simple and yields good results with common datasets. We present a technique that combines heterogeneous tree algorithms and contrast it with homogeneous forest algorithms. Our results indicate that random forests do poorly when faced with irrelevant attributes, while our heterogeneous technique handles them robustly. Further, we show that large ensembles of random trees are more susceptible to diminishing returns than our technique. We are able to obtain better results across a large number of common datasets with a significantly smaller ensemble.},
	booktitle = {Seventh {International} {Conference} on {Machine} {Learning} and {Applications}, 2008. {ICMLA} '08},
	author = {Gashler, M. and Giraud-Carrier, C. and Martinez, T.},
	month = dec,
	year = {2008},
	keywords = {accuracy, Algorithm design and analysis, Application software, Bagging, Computer science, decision trees, Diversity reception, machine learning, robustness, Training data},
	pages = {900--905},
	file = {IEEE Xplore Abstract Record:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/ZADWDQMT/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/A2KMI5RD/Gashler et al. - 2008 - Decision Tree Ensemble Small Heterogeneous Is Bet.pdf:application/pdf}
}

@book{wasserman_all_2004,
	address = {New York},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {0387402721  9780387402727},
	shorttitle = {All of statistics},
	abstract = {"This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining, and machine learning." "This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping, and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate levels."--BOOK JACKET.},
	language = {English},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	file = {All Of Statistics [Wasserman].pdf:/home/johnros/.zotero/zotero/66g0wvis.default/zotero/storage/R7TU8M27/All Of Statistics [Wasserman].pdf:application/pdf}
}