
\chapter{Unsupervised Learning}

\label{sec:unsupervised}


\section{Introduction to Unsupervised Learning}

Unlike Supervised Learning, in Unsupervised Learning there is no outcome variable $y$.
Unsupervised learning is thus not a task per-se, but rather a learning setup, which may include many different tasks:
\begin{enumerate}
\item \textbf{Density Estimation}: Estimate $\dist(x)$.

\item \textbf{High Density Regions}: 
Find feature combinations which tend to concentrate, hopefully, because the belong to homogenous and interpretable subgroups.

\item \textbf{Visualize}: 
Visually inspect the data. 

\item \textbf{Cluster}: 
Assign observation to homogenous groups, i.e., clusters.
Alternatively, assign not only observations, but also unobserved values. I.e., learn a \emph{clustering function}.
\end{enumerate}


When thinking of machine learning as the means to automate decision processes, then unsupervised learning is typically as a pre-processing stage before a supervised learning. 

We will see that the building blocks presented in the introduction (\S\ref{sec:introduction}) will make an appearance in practically every task and very algorithm. Namely, dimensionaly reduction (Appendix~\ref{apx:dim_reduce}), the kernel trick (Appendix~\ref{apx:rkhs}), the spectral trick (Apendix~\ref{apx:spectral}) and generative approaches (Appendix~\ref{apx:generative_concept}).






\paragraph{Graph Method or Feature Methods?}
As we will see, some methods only require a similarity graph ($\similaritys$), while other require the underlying features ($X$).
What is better?
Well, there is no single answer to the question.
Here are some pros and cons:
\begin{enumerate}
\item Computational complexity: 
Storing $X$ requires $n \times p$ memory units\footnote{ I hope that readers familiar digital storage will forgive my non-rigorous treatment of storage modes.}.
Storing $\similaritys$ will require $n \times n$ memory units.
Most algorithms have \emph{streaming} versions. Meaning that the solutions can be iteratively obtained by processing only parts of $X$ or $\similaritys$ at a time. \marginnote{Streaming Algorithm}
The computational complexity of each method can thus vary depending on $n$,$p$, and the particular algorithm implementing the method. 
No approach dominates the other.

\item Data reconstruction or space reconstruction:
Methods taking $\similaritys$ inputs, return an embedding. They thus apply only to the observed points.
Methods taking $\featureS$ typically return an embedding \emph{function}. The result can thus effortlessly be applied to new points. Put differently, they embed the whole $\featureS$ into $\manifold$ and not only $X$. 
Exceptions do exist. PCA (\S\ref{sec:pca}) for instance, can take $\similaritys$ and return an embedding function.
\end{enumerate}





\begin{remark}[Unsupervised Learning in the ERM framework]
Many unsupervised learning problems, can be cast in the ERM framework.
We currently do not call it ERM, but essentially every optimization problem we will see can be thought of as the minimization of some loss, over some hypothesis class.
This has not been done in the current version of our text due to both time constraints, and compatibility with the referenced literature.
\end{remark}




% % % Density Estimation % % % % 
\section{Density Estimation}
\label{sec:density_estimation}

We now aim at the possibly hardest target in unsupervised learning- estimating $\dist(x)$.
As previously stated, learning $\dist$ is of interest for itself, but can also serve for classification (\S\ref{sec:generative}), for detecting high density regions (\S\ref{sec:high_density}), and for clustering (\S\ref{sec:cluster_analysis}).




\subsection{Parametric Density Estimation}
If a parametric generative model can be assumed, this collapses to the parameter estimation problem in the classical statistical literature (\S\ref{sec:estimation}). Maximum Likelihood estimation being a particularly attractive approach, but certainly not the only one.
If a parametric model cannot be assumed, we fall into the realm of non-parametric methods. 
As we saw for supervised learning, these typically rely on pooling information from neighbourhoods of $\featureS$.



\subsection{Kernel Density Estimation}
\label{sec:kernel_density}

Much like the Kernel Regression regression, a \naive estimator is the moving average.
A natural generalization, in the spirit of the Nadaraya-Watson smoother (Eq.(\ref{eq:nadaraya_watson})) is the \emph{Parzen} estimate:\marginnote{Parzen Estimate}
\begin{align}
\label{eq:parzen}
	\estim{\pdf}(x):=\frac{1}{n \lambda} \sum_{i=1}^n \kernel_\lambda(x,x_i).
\end{align}

\begin{remark}
If you have been convinced by the use of KNN (\S\ref{sec:knn}) for regression (or classification), there is no reason not to use it for density estimation. It will keep offering the same pros and cons as in KNN regression (or classification).
\end{remark}

As previously stated, these methods may fail when $x$ are high dimensional. We thus recur to other methods.



\subsection{Graphical Models}
\label{sec:graphical_models}

Graphical models are graphs that represent joint distributions\footnote{Unlike \emph{graph data}, where the graph is the data itself, and not a distribution.}
For our purposes, we discuss only \emph{undirected graphical models}, also known as \emph{Markov random fields}, or \emph{Markov networks}.\marginnote{Markov Random Field}
The reason these models are interesting, and particularly for the purpose of density estimation, is that missing edges in a graphical model represents conditional independencies between random variables. 
If the graph structure can be assumed, and hopefully has many missing edges, then the dimension of the density estimation problem is considerably reduced. 

\begin{example}[First Order Univariate Markov Process]
Consider a Gaussian random vector $\x \sim \gauss{\mu, \Sigma}\in \reals^p$.
To see how the graphical model, i.e. the Markov assumption, helps us with the estimation let's count the number of estimated parameters.
Without any further assumptions, there are $p$ parameters in $\mu$ and $p(p+1)/2$ parameters in $\Sigma$ to estimate.
This corresponds to a fully connected graph of the distribution.
If, however, we can assume a first order Markov structure, implying that the graph of the distribution is merely a chain, there are less parameters to estimate: $p$ parameters in $\mu$ and $p+(p-1)$ parameters in $\Sigma^{-1}$. All other entries in $\Sigma^{-1}$ vanish.
Say $p=10$, then the first order graphical model has $29$ parameters, compared to $65$ in the fully connected graphical model.


\end{example}


In the cases the graph structure cannot be assumed, then our goal becomes the learning of the graph structure from the data. I.e., learning the (conditional) independencies between the variables. For a multivariate Gaussian distribution, this amounts to estimating the covariance, while imposing some sparsity assumption. 
An important fact in this regard is that in the multivariate Gaussian case, missing edges in the graphical model, mean zero entries in the inverse covariance matrix. This fact sets the bridge between learning the structure of a graphical model, and (sparse) covariance estimation.


\begin{remark}[Restricted Bolzmann Machine]
In this context it is also worth mentioning the Restricted Boltzmann Machine (RBM), which is a multivariate distribution with particular class of graphical structure, motivated from Neural Networks. \marginnote{Restricted Bolzmann Machine}
\end{remark}


For a more detailed review of Graphical Models, see Chapters 17 and 20 in \cite{wasserman_all_2004}. 






% % % % High density regions % % % % %
\section{High Density Regions}
\label{sec:high_density}

When data is high dimensional a full blown density estimation may be statistically and computationally impractical.
We can still, however, aim at detecting regions in $\featureS$ with high probability (which would have been trivial have we had a density estimate).

Once high density regions have been detected, they could be assigned to clusters. Then again, assigning to clusters is typically an easier problem which can be solved directly (see \S\ref{sec:cluster_analysis}).



\subsection{Association Rules}
\label{sec:association}
Association rules, or \emph{market basket analysis}, or \emph{affinity analysis}, can be seen as approximating the joint distribution with a region-wise constant function (Eq.~\ref{eq:decision_list}).\marginnote{Market Basket Analysis}
Put differently, we want to capture high density regions of the joint distribution of $x$ with by approximating it with a decision-list (not tree).
Learning a decision-list is a computationally impractical problem in general. Association rules are thus typically learned over binary feature spaces $\featureS$, using heuristic optimization schemes.

This type of problems typically occurs in sales analysis, where vendors seek for combinations of products that tend to sell jointly (so that can design the store better, or discount product bundles).

The \emph{Aprioiri} algorithm \cite{agraval_fast_1994}, is an example of such a heuristic search for high density combinations.\marginnote{Apriori Algorithm}

\paragraph{Terminology}
[TODO: update terminology in terms of probabilities and not supports.]
The following terms relate to the discussion of rules of the type ``if $A$ then $B$''. Example, ``if beer then diapers''.
\begin{itemize}
\item \textbf{Antecedent \& Consequent}: For a rule ``if $A$ then $B$'', or $A \Rightarrow B$, $A$ is the antecedent and $B$ is the consequent.
\item \textbf{Support}: The empirical probability of an item, $\supp{A}:=\sum \indicator{A \in \sample}/n$. The support of \emph{a rule}, is the support of the item combination: $\supp{A \Rightarrow B}:=\sum \indicator{A \union B \in \sample}/n$.
\item \textbf{Confidence}: $\conf{A}{B}:=\frac{\supp{A \union B}}{\supp A}$. A strong rule has confidence close to $1$, meaning the rule applies with almost certainty.
\item \textbf{Lift}: $\lift{A}{B}:=\frac{\supp{A \union B}}{\supp A \supp B}$. A strong rule has a large lift, meaning the items typically come in combos.
\item \textbf{Conviction}: $\convic{A}{B}:=\frac{1-\supp{B}}{1-\conf{A}{B}}$. It can be interpreted as the probability of a rule to make a wrong prediction.
\end{itemize}

















\section{Linear-Space Embeddings}
\label{sec:dim_reduce_linear}

Linear space embeddings are a class of dimensionality reduction techniques that map the data $X$ into a lower dimensional \emph{linear} space $\manifold$. 
The mapping itself, $\hyp: \featureS \to \manifold$ can be linear or non linear.
We denote the low dimensional representation of the data by $\hat{X}:=\hyp(X) \in \manifold$.

The idea of ERM and Inductive Bias also applies to unsupervised learning.
We seek some $\hyp$ that does not incur too much loss, on average. I.e., we seek to minimize $\risk(\hyp)$.
As usual, we do not have access to the data generating process of $x$, so we typically content ourselves with the empirical risk minimization.
In the context of unsupervised learing, the empirical risk is known as the \emph{reconstruction error}.\marginnote{Reconstruction Error}

For a general discussion of the idea of dimensionality reduction see Appendix~\ref{apx:dim_reduce}.

Many dimensionality reduction methods, if not all, take inspiration from the origins of PCA. 
It is thus strongly recommended to read the PCA section (\S\ref{sec:pca}) before proceeding.

\begin{remark}[Interpreting ``Linear'']
\label{remark:linear}
	Two interpretations of ``linear'' can be found in the literature. It may refer to the nature of the low dimensional space approximating the data, or to the nature of the embedding operation. In this text, we use the first interpretation, meaning that $\manifold$ is a linear subspace, even if the embedding operation is non linear.
\end{remark}


\begin{remark}[Interpreting the Low Dimensional Representation]
I we are not only interested in a compressed representation of our data, but also in assigning some intuitive meaning to this compressed representation, we need to be cautious. 
An example of the difficulties that may arise try comparing the PCA problem (\S\ref{sec:pca}) with the factor analysis problem (\S\ref{sec:factor_analysis}).
For interpretation purposes, it may be preferable, in the spirit of statistical inference, to assume some interpretable generative model and approach the dimensionality reduction as a statistical inference problem.
This is the theme of the Latent Space Generative Models section (\S\ref{sec:latent_space}).
\end{remark}




\subsection{Principal Components Analysis (PCA)}
\label{sec:pca}
\id{dim reduce}{optimization}{$\similaritys$}{embedding function}{}{}


PCA is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of \emph{discrete Karhunen–Loève Transform; Hotteling Transform; Proper Orthogonal Decomposition (POD); Eckart–Young Theorem; Schmidt–Mirsky Theorem;  Empirical Orthogonal Functions; Empirical Eigenfunction Decomposition;  Empirical Component Analysis;  Quasi-Harmonic Modes;  Spectral Decomposition;  Empirical Modal Analysis;} and possibly more\footnote{Wikipedia: \url{http://en.wikipedia.org/wiki/Principal_component_analysis} }.
The many names are quite interesting as they offer an insight into the different problems that led to its (re)discovery.

Starting with an example, consider human height and weight data. 
While clearly two dimensional data, you don't really need both to understand how ``big'' are the people in the data. 
This is because, height and weight vary mostly along a single dimension, which can be interpreted as the ``bigness'' of an individual. 
This is why, physicians use the Body Mass Index (BMI) as an indicator of size, instead of a two-dimensional measurement.
Assume you now wish to give each individual a size score, that is a linear combination of height and weight: PCA does just that. It returns the linear combination that has the most variability, i.e., the combination which best distinguishes between individuals. 

The variance maximizing motivation above was the one that guided Hotelling \citep{hotelling_analysis_1933}.
About $30$ years before him, Karl Pearson \citep{pearson_liii._1901} derived the same procedure with a different motivation in mind. Pearson was also trying to give each individual a score. He did not care about variance maximization, however. He simply wanted a small set of coordinates in some (linear) space that approximates the original features well. As it turns out, the best linear-space approximation of $X$ is also the variance maximizing one. Pearson and Hotelling thus arrived to the same solution, with different motivations. 





\paragraph{Terminology}
PCA has received much attention. As such, it has rich underlying theory and terminology.
Here are some terms needed to understand PCA outputs:
\begin{itemize}
\item \textbf{Principal Components}:  The linear combinations of the features, which best separate between observations. In our example- the ``bigness'' index of each individual. The first components captures the most variance, the second components, the second most-variance, etc. In terms of $\manifold$, the principal components are an orthogonal basis for $\manifold$.
\item \textbf{Scores}: Synonymous to Principal Components.
\item \textbf{Loadings}: The weights of each data point in each principal component. In our example, the importance of the height and weight in constructing the ``bigness'' score.
\end{itemize}


\subsubsection{Scree Plot}
\label{sec:scree_plot}
[TODO]



\subsubsection{Bi Plot}
\label{sec:bi_plot}
[TODO]



\subsubsection{Intuition}
\label{sec:pca_intuition}

Notice we have currently offered two motivations for PCA: 
(i) Find linear combinations that best distinguish between observations, i.e., maximize variance. 
(ii) Find the linear subspace the bets approximates the data.
The reason these two problems are equivalent, is due to the use of the squares error.
Informally speaking, the data has some total variance. This variance can be decomposed into the part captured in $\manifold$, and the part not captured\footnote{Analogous to $SST=SSR+SSE$ in linear regression.}. 
Since the variance in the data consists of sums of squares, minimizing the distance from $X$ to $\manifold$, is the same as maximizing the variance of $X \project \manifold$, since their sum is fixed.



\subsubsection{Mathematics of PCA}
\label{sec:pca_mathematics}
We now present the derivation of PCA from the two different motivations.




\paragraph{The Variance Maximization View}
Starting with the first principal component.
If $\cov{x}=\Sigma$ then for a fixed vector $v$: $\cov{vx}=v \Sigma v'$.
Maximizing w.r.t. $v$ does not make sense as $\cov{vx}$ will explode. 
It is most convenient, mathematically, to constrain the $l_2$ norm: $\normII{v}^2=1$.
Maximizing under a constraint, using Lagrange-Multipliers: 
\begin{align}
 \argmax{v}{v \Sigma v' - \lambda (\normII{v}^2-1)}.
\end{align}
Differentiating w.r.t $v$ and equating zero: 
\begin{align}
	(\Sigma- \lambda I)v'=0
\end{align}
So the $P$ solutions for $v$ are the eigen-vectors of $\Sigma$. Which of them to pick? 
To find a \emph{global} maximum we return to the original problem, as plug our result:
\begin{align}
\label{eq:pca_maximal_variance}
 \argmax{v:\normII{v}^2=1}{v \Sigma v' }=\argmax{\lambda}{v \lambda v' }
\end{align}
so that the global maximum is obtained with the largest eigen-value $\lambda$.

Readers familiar with matrix norms will recognize that this is simply the derivation of the operator norm of $\Sigma$.

The second principal component can be found by solving the same problem, with the additional constraint of $v_2$ orthogonal to $v_1$.

The last missing ingredient is that instead of the true covariance between the features, $\Sigma$, we use the (scaled) empirical covariance $X'X$.




\paragraph{The Linear-Space Embedding View}
We now seek to find a sequence of $p$ approximations to $X$ that lay in $1,\dots,p$ dimensional linear subspaces, with respect to a least squares loss. For simplicity of exposition, we will assume that $X$ has been mean centred. 
The $\rank$'th problem to solve is thus
\begin{align}
\label{eq:pca_erm}
	\argmin{\hyp_\rank}{\normF{X-\hyp_\rank(X)}}.
\end{align}
Since $\hyp_rank$ is a map from $\reals^p$ to some rank-$\rank$ linear subspace, it must have the form $\hyp_\rank(X)=\projectMat_\rank X$ where $\projectMat_\rank$ is a $n \times n$ matrix of rank $\rank$.
Since Eq.(\ref{eq:pca_erm}) minimizes sums of (squared) Euclidean distances, $\projectMat_\rank$ has to be an orthogonal projection, thus symmetric. As such it can decomposed into an outer product $\projectMat_\rank=V_\rank V'_\rank$ where $V_\rank$ is full rank $n \times \rank$ matrix \citep[Eq.(5.13.4)]{meyer_matrix_2001}.
Under the $\rank$-space constraint, and squared error, Eq.(\ref{eq:pca_erm}) collapses to 
\begin{align}
\label{eq:pca_erm2}
	\argmin{V_\rank}{\normF{X-V_\rank V'_\rank(X)}}.
\end{align}
Using some algebraic identities \cite[Eq.(23.3)]{shalev-shwartz_understanding_2014} Eq.(\ref{eq:pca_erm2}) is equivalent to 
\begin{align}
\label{eq:pca_erm3}
	\argmax{V_\rank}{\Tr(V'_\rank XX' V_\rank)}.		
\end{align}
At this point we should note that the linear-space embedding problem has collapsed to the variance maximization problem! 
If you do not see this, just set $\rank=1$ and compare to Eq.(\ref{eq:pca_maximal_variance}), recalling that $X'X$ estimates the features' covariance $\Sigma$.






\subsubsection{PCA as a Graph Method}
\label{remark:pca_as_graph}
Starting from the maximal variance motivation, it is perhaps not surprising that PCA depends only on the similarities between features, as measured by their empirical covariance. The linearity of the target manifold was there by assumption. 

Following the linear-space embedding motivation, it is was surprising that the solutions depend only on the empirical covariances. This fact can be attributed to the use of squared error loss, which implied we were trying to decompose the total variance into the part in $\manifold_\rank$ and the orthogonal part.

From both motivations we see that the values of $X$ are of no importance given $X'X$, which can be informally thought of as a sufficient statistic\footnote{It is not a proper sufficient statistic as no generative model has been assumed.}.  

In-turn, $X'X$ depends only on the empirical covariances between \emph{individuals} ($\similaritys=XX'$), or on the Euclidean distances between individuals ($\dissimilaritys=(\norm{x_i-x_j})$).

The building blocks of all these graph-based dimensionality reduction methods are:
\begin{enumerate}
\item Compute some similarity graph $\similaritys$ (or dissimilarity graph $\dissimilaritys$) from the raw features.
\item Call upon graph embedding theory to map the data points into the target manifold $\manifold$.
\end{enumerate}
The fact that the linear-space embedding of the data depends only some similarity graph has laid a bridge between feature embedding, such as PCA, and \emph{graph embedding} methods such as MDS (\S\ref{sec:mds}).
Moreover, it has opened the door for replacing the covariance similarity, with many other similarity measures. 
Classic MDS (\S\ref{sec:mds}) is simply PCA when starting from $\similaritys$, thus viewed as a graph embedding problem.
kPCA (\S\ref{sec:kpca}) plugs kernel similarities (\S\ref{apx:rkhs}) instead of covariance similarities. 
Isomap (\S\ref{sec:isomap}), LocalMDS (\S\ref{sec:localMDS}), and LLE (\S\ref{sec:lle}) follow a similar motivation using \emph{local} measures of similarity.
Spectral Clustering (\S\ref{sec:spectral_clustering}) does some linear-space embedding \`a-la PCA, then wrapping up with a clustering algorithm in $\manifold$ \`a-la K-means. 



We now prove that the PCA solution can be cast in terms of the covariance between individuals ($\similaritys=XX'$) or the Euclidean distances ($\dissimilaritys=\norm{x_i-x_j}$).
In particular, we show that all the information on the location (mean) of $X$, needed for the PCA reconstruction, is actually encoded in $\similaritys$ (or $\dissimilaritys$).

The following exposition takes from \cite[Section 18.5.2]{hastie_elements_2003}


\paragraph{PCA with the Covariance Similarity Graph}
To begin, we need to cast the solution to the PCA problem in Eq.(\ref{eq:pca_erm3}) using the Singular Value Decomposition (SVD).\marginnote{SVD}

\begin{definition}[SVD]
Any $n \times p$ matrix $X$, can be decomposed into $X=UDV'$ where 
$U$ is an $n \times p$ orthogonal matrix ($U'U=I_p$); 
$D$ is a $p \times p$ diagonal matrix with diagonal elements $d_1 \geq d_2 \geq \dots \geq d_p$;
$V$ is a $p \times p$ orthogonal matrix ($V'V=I_p$).
\end{definition}

For mean centered $X$, the series of embeddings $\hyp_\rank(X)$ for $\rank=1,\dots,\pagebreak$ resulting from Eq.(\ref{eq:pca_erm3}) is given by $\hyp_\rank(X)=U_\rank D_\rank$, where $U_\rank$ $D_\rank$ are the $\rank$ leading columns of $U$ and $D$ respectively. $UD$ is thus the sequence of all solutions.

Now denoting $\similaritys=XX'$ and calling SVD: $\similaritys=U D^2 U'$. 
We thus see that by decomposing $\similaritys$ we can recover $U$, $D$, and thus $\hyp_\rank(X)$.

If $X$ is not mean centred, the relation still holds, but we skip the presentation.


\paragraph{PCA with the Euclidean Distance Dissimilarity Graph}
Can we convert Euclidean distances to empirical covariances? Yes!

Denote the matrix of distances of a non-centred $X$: $\dissimilaritys^2=(\norm{x_i-x_j}^2)$.
\begin{align}
	\dissimilaritys^2_{i,j} =& \norm{x_i-x_j}^2 \\
	=& \normII{x_i-\bar{x}}+\normII{x_j-\bar{x}}-2 \scalar{x_i-\bar{x}}{x_j-\bar{x}} \\
	=& \normII{x_i-\bar{x}}+\normII{x_j-\bar{x}}-2 \similaritys_{i,j}
\end{align}
where $\similaritys_{i,j}$ is the empirical covariance between individual $i$ and $j$.
We thus have 
\begin{align}
	\similaritys= - (I-M) \frac{\dissimilaritys^2}{2} (I-M)
\end{align}
where $M$ is the centring matrix: $M:= \frac{1}{n} \ones \ones'$, and $\ones$ an $n$ vector of $1$'s.




\subsection{Random Projections}
\label{sec:random_projections}

What if instead of optimizing a linear embedding of the features with respect to some criterion, such as PCA (\S\ref{sec:pca}), or FA (\S\ref{sec:factor_analysis}), we simply apply a random linear mapping $W X$. How bad will the distances between the observations be distorted? 
It turns out that not too much!
The Johnson-Lindenstrauss Lemma \citep{johnson_extensions_1984} quantifies this distortion, essentially implying that we may reduce the dimension of our data in a very \naive manner, while still conserving pair-wise similarities $\similaritys$ between observations.
 




\subsection{Sparse Principal Component Analysis (sPCA)}
\label{sec:spca}

When analyzing the PCA results, we often with to understand which features contribute to which component. 
This is much easier when the loadings, $\loadings$ are sparse, i.e., include many zeroes. 
This is the purpose of sPCA. 
We will not go into the technical detail, but merely state that sPCA performs this, \`a-la LASSO style, by means of $l_1$ regularization.









\subsection{Multidimensional Scaling (MDS)}
\label{sec:mds}

MDS aims at representing a network\footnote{The term Graph is typically used in this context instead of Network. But a graph allows only yes/no relations, while a network, which is a weighted graph, allows a continous measure of similarity (or dissimilarity). It is thus more appropriate.} of distances (or similarities) between observations, by embedding the observations in a $\rank$ dimensional \emph{linear} subspace, while preserving the original distances.
The network may be obtained by computing some similarity (or dissimilarity) measure with the raw features $X$, or simply because the data itself is a network (social, communication, etc.).

Since MDS mainly serves for visualizing data, it is most natural to use $\rank=2$. 
The embedding will distort the original distances (or\dots), and may even change the ordering of the observations. The good news is that it is easier to visualize \andor cluster them in their new simplified representation. 

If the input of MDS is the empirical covariance similarity network, then MDS with ``classical scaling'' (see below) returns the exact same solution as PCA.

The embedding is merely the assigning of each point to a location in some lower dimensional linear space $\manifold$. 
The assignment is driven by a \emph{stress function} which penalizes for the average distortion created by the embedding.
The different types of MDSs, such as \emph{Classical MDS}, and \emph{Sammon Mappings}, differ in the stress function driving the embedding.

Sadly, MDS may scale poorly to large dissimilarity matrices, and the optimization may converge to a local minimum.
Being a data embedding, and not an embedding function, when new data points are made available, the embedding will have to be re-learned.

For more on MDS see Section 14.8 in \cite{hastie_elements_2003} or \cite{borg_modern_2005}.


\paragraph{Mathematics of MDS}
We start with either a dissimilarity network $\dissimilaritys=(\dissimilarity_{i,j})$, or a similarity network $\similaritys=(\similarity_{i,j})$.
Similarities can be thought of as correlations, and dissimilarities as distances (which are indeed the typical measures in use).
Define $z_i \in \reals^\rank$ the location of point $i$ in the target linear space of rank $\rank$. 
The $z_i$'s are set to minimize some penalty for geometric deformation called the \emph{stress function}.
Typical stress functions include:
\begin{description}

\item[Classical MDS] Using the centred inner product (i.e. empirical covariance) as the similarity measure and minimizes the squared distortion:
$\similarity_{i,j}:= \scalar{x_i-\bar x}{x_j-\bar{x}}$ and the new location are given by
\begin{align}
\label{eq:mds_classical}
	 \argmin{z_1,\dots,z_n}{\sum_{i,j=1}^{n} (\similarity_{i,j}-\scalar{z_i-\bar z}{z_j-\bar{z}} )^2}.
\end{align}

\item[Least Squares] Also known as \emph{Kruskal-Shepard}. Also minimizes the squared distortion. 
\begin{align}
\label{eq:mds_stress}
	 \argmin{z_1,\dots,z_n}{\sum_{i \neq j} (\dissimilarity_{i,j}-\norm{z_i-z_j} )^2}.
\end{align}

\item[Sammon Mapping] Also known as \emph{Sammon's stress}, aims at minimizing the \emph{proportion} of distortion:
\begin{align}
\label{eq:mds_sammon}
	 \argmin{z_1,\dots,z_n}{\sum_{i \neq j} \frac{(\dissimilarity_{i,j}-\norm{z_i-z_j} )^2}{\dissimilarity_{i,j}}}.
\end{align}

\end{description}


\begin{remark}[Classical and Least Squares MDS]
Although they both minimize the squared distortion, working with $\similaritys$ or $\dissimilaritys$ lead to different solutions. 
In particular, Classical is a linear embedding while Stress is not. 
\end{remark}




\subsection{Local MDS}
\label{sec:localMDS}
Local MDS is motivated by the observation that if the data does not lay in a globally convex subspace, then global distances are a very distorted measure, whereas geodesic distances should be used instead. 
Their solution is to compute $\dissimilaritys$ using only local distances, and then calling upon MDS.

\begin{remark}[The Non-Linearity of Local MDS]
\label{remark:linearity_of_localMDS}
Local MDS is typically considered a non-linear-space embedding, thus belonging to Section \ref{sec:dim_reduce_nonlinear}.
I currently do not think is the case, as it is presented as a linear space embedding.
Maybe Remark \ref{remark:nldr} can explain the confusion in terminology.
\end{remark}



\subsection{Isometric Feature Mapping (Isomap)}
\label{sec:isomap}

Isomap, also known as \emph{principal coordinate analysis}, is another method intimately related to MDS(\S\ref{sec:mds}).\marginnote{Principal Coordinate Analysis}

Isomap follows the same motivation as Local MDS (\S\ref{sec:localMDS}), but with a different algorithm to compute the dissimilarity matrix $\dissimilaritys$.

\begin{remark}[The Non-Linearity of Isomap]
Just like Local MDS, Isomap is typically considered a non-linear-space embedding (see Remark \ref{remark:linearity_of_localMDS}).
I currently do not think is the case.
\end{remark}









% % % % Non linear dimensionality reduction % % % % %
\section{Non-Linear-Space Embeddings}
\label{sec:dim_reduce_nonlinear}


Section \ref{sec:dim_reduce_linear} deals with representing the data in a \emph{linear} sub space $\manifold$. They all aim at finding a basis which efficiently represents that data, with respect to some target function. 
In this section, we allow $\manifold$ to be non-linear. 
We will thus no longer be able to represent the data by its coordinates in some basis.


\begin{remark}[Non Linear Dimensionality Reduction]
\label{remark:nldr}
	Do not let the title of this section be confused with the term Non-Linear Dimensionality-Reduction (NLDR).
	NLDR deals with the nature of the \emph{embedding} operation, and not with the structure of the target manifold $\manifold$ (see also Remark \ref{remark:linear}).\marginnote{NLDR}
	This section deals with embeddings into a non-linear subspace, regardless of the nature of the embedding operator.
	Cases of non-linear embeddings (NLDR) into a \emph{linear} manifolds $\manifold$ belong in Section~\ref{sec:dim_reduce_linear}.
\end{remark}




\subsection{Kernel Principal Component Analysis (kPCA)}
\label{sec:kpca}

Back to the motivating example from the PCA section (\S\ref{sec:pca}): assume we want to construct a ``bigness'' score, that best separates between individuals, but we no longer constrain it to be a linear function of the height and weight.
Recalling that the best discrimination between observations means maximizing the variance of the \emph{scores} given to individuals, we could try to find the best separating score $g(x)$ by solving 
\begin{align}
\label{eq:kpca_wrong}
	\argmax{g}{\covn{g(X)}}
\end{align}
where $g(x)$ maps an individual's features to a score in $\manifold$.

Alas, just like in the supervised learning problem, without any constraints on $\manifold$, thus on $g$, we might overfit \andor not be able to compute $g$ as optimization is done in a infinite dimensional space. 
We thus have two matters to attend:
(i) We need to constrain $g(x)$ so that it does not overfit.
(ii) We need the problem to be computable.
This is precisely the goal of kPCA. 

We have already encountered a similar problem with Smoothing Splines (\S\ref{sec:smoothing_splines}). It is thus not surprising that the solution has the same form. 
Namely, if we choose the right $g$'s, the solution of Eq.(\ref{eq:kpca_wrong}) takes a very simple form. 
The classes of such $g$'s are known as Reproducing Kernel Hilbert Spaces (RKHS). 
They are discussed in Appendix~\ref{apx:rkhs}.



\paragraph{Mathematics of kPCA}
[TODO]




\subsection{Self Organizing Maps (SOM)}
SOMs, are a non-linear-subspace dimensionality reduction method, aimed at good clustering. 
It is non-linear because the algorithm (which cannot be cast as an ERM problem) returns an embedding into a non-linear manifold.
More details in Section~\ref{sec:som}.



\subsection{Principal Curves and Surfaces}
\id{dim. reduce}{algo.}{$X$}{parametric curve or surface}{self consistency}{}

Principal curves (or surfaces), is an algorithm. It cannot be cast as an optimization problem.
Being a non-linear space embedding method, the algorithm iterates until it returns a curve of a surface.
In the curve case, it will return a curve with the \emph{self consistency} property. I.e., a curve with a path that is the average of all it's closest data points.\marginnote{Self Consisntecy}
Roughly speaking, one can think of this curve as a parametrized function, connecting all the k-means cluster centres in the smoothest way possible.
Using the same, slightly inaccurate depiction, a principal surface is a \emph{surface} connecting these k-means cluster centres.
We stress that in both cases, the output is a continuous parametrization of the curve or the surface.

It is highly uncommon to approximate the data with surfaces (manifolds) with a dimension larger than $2$, as typically the method is used for projecting the data before visualizing \andor clustering.






\subsection{Local Linear Embedding (LLE)}
\label{sec:lle}
\id{dim. reduce}{algo.}{$\similaritys$}{data embedding}{local distance}{}


LLE aims at finding linear subspaces that are good approximations of small neighbourhoods of the whole data $X$.
It is similar in spirit to Isomap (\S\ref{sec:isomap}) and LocalMDS (\S\ref{sec:localMDS}).
It differs, however, in the way similarities are computed, and in the way embeddings are performed. 
In particular, as the name may suggest, LLE performs local embeddings to linear subspaces. The resulting approximating manifold $\manifold$, being the ``stitching'' of many linear spaces, is ultimately non linear.




\subsection{Auto Encoders}
\label{sec:auto_encoders}
[TODO. AKA auto-associator or Diabolo network]



\subsection{Information Bottleneck}
[TODO]

%The \emph{information bottleneck} is an information theoretic framework, due to \cite{tishby_information_1999}, that generalizes the idea of dimensionality reduction and data compression.
%It is a general statement of the problem of maximal compression of $X$, while preserving as much of the information in $X$ as possible.
%To state the general problem we require some definitions from information theory.
%
%\begin{definition}[Entropy]
%The entropy of a random variable $\x$ is defined as the expected negative log density of $\x$: 
%$$\expect{-\log \pdf(\x)}.$$
%\end{definition}
%
%\begin{definition}[Mutual Information]
%The mutual information between two random variable $\x$ and $\y$ is defined as : 
%$$\expect{\log \frac{\pdf(\x,\y)}{\pdf(\x) \pdf(\y)}}.$$
%\end{definition}
%
%Informally speaking, the entropy is a measure of the variability of a distribution, and the mutual information is a measure of deviation from independence, i.e., captures the amount of information $\x$ carries on $\y$, and vice-versa.
%With this intuition, and denoting the compression of $x$ by $y=g(x)$, we may define the compression problem as:
%\begin{align}
%	\argmin{g}{arg2}
%\end{align}


\begin{remark}[Information Bottleneck and ICA]
[TODO]
\end{remark}












%%%%%%%%% Generative Models %%%%%%%%%%%



\section{Latent Space Generative Models}
\label{sec:latent_space}

We have already met generative models for supervised learning (\S\ref{sec:generative}), and for unsupervised learning (\S\ref{sec:density_estimation}).
We now discuss a class of generative models, that can be seen as a dimensionality reduction device. 
The following models and methods assume that the data generating process is governed by some low dimensional unobservable \emph{state}, also known as \emph{latent variable}, or \emph{hidden variable}.
This is why these models are also known as \emph{state space} models. A term coined by  \citet{kalman_contributions_1960}.\marginnote{State-Space Models}

The simplification of the generative model from its original high-dimension allows us to:
(i) \emph{Interpret} the data generating process via its states. 
(ii) \emph{Formulate} the density estimation problem as a low dimensional problem, we can actually hope to solve, even when the data itself is very high dimensional.
(iii) \emph{Visualize} the data. 

The fundamental idea of latent space generative models is that while the data generating distribution may have a complicated form, when we condition on the unobserved variable, the distribution greatly simplifies. 



\subsection{Factor Analysis (FA)}
\label{sec:factor_analysis}
\id{dim reduce}{optimization}{$X$}{embedding function}{}{}

Examples \ref{eg:intelligence} and \ref{eg:face_rotations} motivate factor analysis, and many other latent space generative models in this section.

\begin{example}[Intelligence Measure (g-factor)]
\label{eg:intelligence}

Assume respondents answer $p$ questions: $x_i \in \reals^p$. 
Also assume, their responses are some linear function $\loadings \in \reals^p$ of a single attribute, $s_i$. 
We can think of $s_i$ as the subject's ``intelligence''.
We thus have 
\begin{align}
	x_i = s_i A + \varepsilon_i
\end{align}
\end{example}



\begin{example}[Face Rotations]
\label{eg:face_rotations}
[TODO]
\end{example}




Factor Analysis is solved very similarly to PCA (\S\ref{sec:pca}), so that the two are often confused. 
FA, however, stems from a rather different motivation than PCA.
PCA is motivated by finding variable combinations (scores) with most variance. 
FA is a generative method, aimed at finding uncorrelated latent attributes. 

In FA we assume that the observed $X$'s depend linearly on a set of $\rank$ independent latent (i.e. unobservable) attributes we denote with $\latent$.
The generative model is thus
\begin{align}
\label{eq:factor}
	X=\loadings \latent+\varepsilon
\end{align}
Assuming a generative distribution on $\latent$ and $\varepsilon$, we may try to estimate $\loadings \latent$ by maximum likelihood.
Recovering the particular latent attributes $\latent$ from $\estim{\loadings\latent}$ is still impossible as there are infinitely many such solutions. To see this, consider an orthogonal \emph{rotation} matrix $\rotation$ ($\rotation' \rotation=I$). For each such $\rotation$: $ \loadings \latent=\loadings \rotation' \rotation \latent = \loadings^* \latent^*$.

The arbitrary choice of $\rotation$ changes the interpretation of the latent attributes. This is why many researchers find FA an unsatisfactory inference tool.

\begin{remark}[Identifiability in PCA]
	The non-uniqueness (non-identifiability) of the FA solution under variable rotation is never mentioned in the PCA context. Why is this?
	This is because the methods solve different problems. Indeed, the PCA problem has no such ambiguity. 
	Generative latent variables, as in FA, are only defined up to rotations. They are thus not unique. 
	Combinations with maximal variance, or series of linear-hyperspaces, as in PCA, are unique. There is no room for rotations.
\end{remark}



\paragraph{FA Terminology}
\begin{itemize}
\item \textbf{Factors}: The unobserved attributes $\latent$. Not to be confused with the \emph{principal components} in the context of PCA.
\item \textbf{Loadings}: The $\loadings$ matrix; the contribution of each attribute to the observed $X$.
\item \textbf{Rotation}: An arbitrary orthogonal re-combination of the latent attributes $\latent$ and loadings, which changes the interpretation of the result.
\end{itemize}


For a brief review of Factor Analysis see \cite{hastie_elements_2003}.
For an full exposition, and a discussion of the differences with PCA, see \cite{jolliffe_principal_2002}.



\paragraph{Rotations}
\begin{itemize}
\item \textbf{Varimax}: By far the most popular rotation. Attempts to construct factors that are similar to the original variables, thus facilitating interpretation. This can be seen as a "soft" approach to sPCA (\S\ref{sec:spca}).
\item \textbf{Quartimax rotation}: Seeks a minimal number of factors to explain each variable. May thus result factors that are uninterpretable, since they all rely on the same variables.
\item \textbf{Equimax rotation}: A compromise between Varimax and Quartimax. 
\item \textbf{Direct oblimin rotation}: Relaxes the requirement of the factors to be uncorrelated, so that they may be similar to the original variables; even more so than in varimax. This facilitates the interpretability of the factors. 
\item \textbf{Promax rotation}: A computationally efficient approximation of oblimin.
\end{itemize}



\begin{remark}[Non Linear FA]
The FA method presented herein deals with features, $X$, that are linear in the latent factors, $\latent$. 
It is obviously possible to generalize the framework to deal with non-linear functions of the latent factors: $X=g(S)$. 
This practice is, however, quite uncommon.
\end{remark}


\paragraph{Mathematics of FA}
[TODO]




\subsection{Independent Component Analysis (ICA)}
\label{sec:ica}
\id{dim reduce}{meta problem}{$X$}{embedding function}{}{}

ICA is a family of latent space models-- a meta-method.
It assumes data is generated as some function of the latent variables $\latent$. 
In many cases this function is assumed to be linear in $\latent$ so that ICA is compared, if not confused, with PCA and even more so with FA. 
In its most popular form, $X$ is assume to be a \emph{linear} function of the latent independent components: $X=\loadings \latent$.

The fundamental idea of ICA is that $\latent$ has a joint distribution of \emph{independent} variables. 
This is a stronger assumption that the typical FA assumption where the distribution of $\latent$ is merely assumed to be uncorrelated. 
This independence assumption solves the the non-uniquness of $\latent$ in FA (\S\ref{sec:factor_analysis}).

Two assumptions distinguish ICA from FA, allowing the identification of $\latent$: 
(i) The latent variables are \textbf{not} Gaussian distributed.
(ii) The latent variables are statistically \emph{independent}.

Being a generative model, estimation of $\latent$ can then be done using maximum likelihood, or other estimation principles. A popular information theoretic estimation principle is \emph{infomax}.\marginnote{Infomax}



ICA is a popular technique in signal processing, where $\latent$ is actually the signal (e.g. sound) produced by several different sources. 
Recovering $\latent$ is thus recovering the original signals mixing in the recorded $X$. This is known as \emph{blind source separation}.\marginnote{Blind Source Seperation}


\begin{remark}[ICA and FA]
The solutions to the (linear) ICA problem can ultimately be seen as a solution to the FA problem with a particular rotation $\rotation$ implied by the independence assumption.
Put differently, the formulation of the (linear) ICA problem, implies a unique rotation. 
\end{remark}

For a general discussion of ICA see \cite{jolliffe_principal_2002}.
For a brief exposition of the linear ICA see \cite{hastie_elements_2003}. 
For a detailed review of ICA see \cite{hyvarinen_independent_2000}. 



\paragraph{Mathematics of ICA}
For ease of presentation we present a simple setup, which can be considerably generalized. 
In this setup, we will first analyze the population problem, i.e., in terms of random variables. 
We thus replace the data $X$, with the random vector $\x$, and afterwards consider implementation for finite samples. 
\begin{itemize}
\item $\x=\loadings \latent$, implying that $\x$ is \emph{linear} in the latent components, and the latent space is of dimension $\rank=p$. It follows that $s=\loadings'X$.
\item $\x$ has been pre-whitented, so that $\cov{\x}=I$.
\item Distance between distributions are measured using the Kullback-Leibler divergence (KL): $\kl{\x}{\latent}$.
\end{itemize}

The optimization problem in this simple ICA is to find an orthogonal matrix $\loadings$, for which:
(i) the components of $\loadings'\x$ are independent;
(ii) $\loadings'\x$ is a good approximation of $\x$.
Formally: 
\begin{align}
\label{eq:ica_optimization}
	\argmin{\loadings \text{ orthogonal} \;; \loadings'\x \text{ independent}}{\kl{\loadings'\x}{\x}}.
\end{align}


By enforcing the independence constraint in Eq.(\ref{eq:ica_optimization}), and due to the properties of the KL divergence, Eq.(\ref{eq:ica_optimization}) is equivalent to
\begin{align}
\label{eq:ica_optimization2}
	\argmin{\loadings \text{ orthogonal}}{\sum_{j=1}^{\rank} \entropy(\loadings_j\x)- \entropy(\x)}
\end{align}
where $\entropy(\x)$ denotes the Entropy of the random variable $\x$ (Definition \ref{def:entropy}).
Now, $\entropy(\x)$ is obviously fixed, so we need to minimize $\entropy(\loadings_j\x)$. 
A classical result in information theory, is that the Gaussian distribution has the maximal entropy. 
Minimizing $\entropy(\loadings_j\x)$ can thus be interpreted as finding a matrix $\loadings$ such that its columns return random variables, $\loadings_j\x$, that are as \emph{non-Gaussian} as possible.

This is where the population analysis ends. 
The insight we take from it, is that finding independent components, is actually finding non-Gaussian combinations of $\x$. 
The different implementations of ICA, indeed look for a matrix $\loadings$ which returns the most non-Gaussian combinations of the observed $X$. 




\subsection{Exploratory Projection Pursuit}
\label{sec:exploratpory_ppr}

\id{dim reduce}{algorithm}{$X$}{embedding function}{}{}

\begin{example}[Intelligence Factor Continued]
\label{eg:intelligence_factor_continued}
Returning to the intelligence score example (\ref{eg:intelligence}).
We seek a single linear combination of answers, a projection $u \in \reals^p$, which helps us visualize, understand, and possibly cluster individuals. 
By assumption, $x_i u = s_i \loadings u + \varepsilon_i u$.
For any $u$ that is orthogonal to $\loadings$ then $\loadings u$ will vanish and the index $x_i u$ is dominated by $\varepsilon_i u$, also close to zero. Moreover, $\varepsilon_i u$ will probably look Gaussian due to the CLT.
For projections $u$ that are similar to $\loadings$, then $\loadings u$ will be large and the different $x_i u$'s will spread nicely on a line, since $\loadings u$ amplifies $s_i$.
\end{example}


The fundamental idea of Exploratory Projection Pursuit, first presented in \cite{friedman_projection_1974}, is that if the data were pure noise in some high dimension, then almost any projection will look like Gaussian noise. 
If, however, the data were not pure noise, then projections in the direction of the signal, would show structure. 



\begin{remark}[Projection Pursuit and ICA]
From the Example~\ref{eg:intelligence_factor_continued} we note that $x_i u$ will look Gaussian for all rotations, $u$, that do not capture signal.
This observation motivated \citet{friedman_projection_1974} to find $u$'s so that they depart from Gaussianity. 
It turns out that the ICA problem (\S\ref{sec:ica}), while having a different motivation, also culminates in finding rotations that maximally depart from Gaussianity.
For this, both ICA and Exploratory Projection Pursuit, can be seen as similar solutions to the problem of finding informative rotations in factor analysis (\S\ref{sec:factor_analysis}).
\end{remark}






\subsection{Compressed Sensing}
\label{sec:compressed_sensing}
[TODO]





\subsection{Generative Topographic Map (GTM)}
\label{sec:gtm}
[TODO]



\subsection{Finite Mixtures}

\id{many}{generative model}{$X$}{estimates}{}{}

A very simply, perhaps the simplest, class of latent variable generative models, is that of Finite Mixtures. 
In a finite mixture, the fundamental assumption is that the observed data is sampled from $K$ unobserved classes. Each class, having some simple joint distribution. 
Being unobserved, we cannot directly learn the conditional simple distributions, without somehow learning the class assignments. 
The likelihood function of such data, is thus the \emph{marginal} probability over all possible classes, thus a \emph{mixture}.
The good news is that a very complex generative model, may be much more easy to learn and interpret if we can assume, or approximately assume, it is actually a finite mixture.

The probability density of a mixture of $K$ classes, each with density $\pdf_k(x)$, is given by  
\begin{align}
\label{eq:mixture}
	\pdf(x)=\sum_{k=1}^{K} \pi_k \pdf_k(x)
\end{align}
where $\pi_k$ is the probability of class $k$.



\begin{remark}[Finite Mixture Distributions]
While very simple to understand, mixture distributions are an unpleasant probabilistic creature. 
Because the probability mass (or density) function is additive in the underlying mixing components, is has a very challenging functional form.
For instance- the likelihood problem will be non-convex with multiple local extrema, so that numerical optimization schemes are not guaranteed to converge.
It may also be the case that the likelihood is unbounded, so that the local extrema are not a global extrema, complicating our attempts at maximum likelihood estimation. 
Hypothesis testing for the number of mixing components is also very challenging as classical statistical theory assumptions do not apply.
\end{remark}


\begin{remark}[Mixtures And the Expectation Maximization Algorithm (EM)]
As previously stated, the likelihood function of mixtures is typically non convex. 
It's optimization is typically done with a numerical algorithm called the Expectation Maximization (EM) algorithm.
Roughly speaking, the idea behind the EM, is that in each iteration observations are assigned to classes, the simple conditional distributions learned, then reasigned using the learnt parameters, etc.\ until convergence.
Even though convergence to an optimum is not guaranteed, it is by far the most popular algorithm for learning mixtures.
\end{remark}



\begin{remark}[Mixtures For Clustering]
Because the output of learning a mixture, is a density like Eq.(\ref{eq:mixture}), a learnt mixture can easily serve for clustering by a simple application of Bayes rule. Simply assign an observation to the cluster with highest (posterior) probability:
\begin{align}
\label{eq:mixture_posterior}
	\prob{x \in k|x}=\frac{\pi_k \pdf_k(x)}{\pdf(x)}
\end{align}
\end{remark}



\begin{remark}[Mixture in Supervise Learning]
The generative models underlying several supervised learning methods, are actually finite mixtures.
Fisher's LDA (\S\ref{sec:lda}) implies a finite mixture of Gaussians, with the same covariance and different means.
Fisher's QDA (\S\ref{sec:qda}) implies a finite mixture of Gaussian, with different covariance and different means.
\Naive Bayes (\S\ref{sec:naive_bayes}) implies a mixture of some distribution, with independent components.
\end{remark}





\subsection{Hidden Markov Models (HMM)}
\label{sec:hmm}
Hidden Markov Models may have two possible interpretations.

The first, and perhaps most common, is as a generalization of the mixture model in the case the generative model assumes dependencies between $x_i$'s and we wish to use a mixture model. 
In this case, we need to model and learn the transition probabilities between states (or classes, are we previously referred to the hidden states of a mixture). 
Assuming the transition between states follows a \emph{Markov process}, i.e., the probabilities of transition depends only upon the current state, the resulting model is an \emph{hidden Markov model}, or \emph{hidden Markov chain}.
\marginnote{Markov Process}

The second, is as a generalization of the Graphical Models (\S\ref{sec:graphical_models}). 
This generalization is known a \emph{latent space graphical model} (\S\ref{sec:latent_graphical}). 
In this case, the Markov property does not describe the process of transition between states, but rather the Markov field property of the conditional distribution in each state, $\pdf_k(x)$. 




\subsection{Latent Space Graphical Models}
\label{sec:latent_graphical}

The idea of simplifying a complicated distribution by conditioning on some latent variable can be compounded with the idea of a graphical model (\S\ref{sec:graphical_models}). 
Put differently, the generative model can be a simple graphical model when conditioned on some latent variable.
See Section 17.4.2 in \cite{hastie_elements_2003} for more details.





\subsection{Matrix Factorization}
\label{sec:matrix_factorization}
[TODO- is it generative?]




% % % % Random Graphs % % % % %
\section{Random Graph Models}
\label{sec:random_graphs}

There are several occasions in which our date does not consist of the classical features $X$, but rather directly given as a similarity graph $\similaritys$. This can happen when the data itself is a graph, such as social networks, communication networks, protein interactions networks, etc. 
It may also be the case that the original data is indeed given as features, then used to compute a similarity graph. 

In those occasion where the data is actually $\similaritys$, we may still want to cluster observations, visualize, etc.

Random Graphs are a class of \emph{generative models}, but unlike other generative methods, the model does not specify a distribution over the features, but rather, a distribution over proximity measures.
For this, it is  intimately related to \emph{random graph} and \emph{random matrix} theory.

For a review of random graph models see \cite{goldenberg_survey_2010}.


\subsection{\erdos \renyi}
[TODO]

\subsection{Exchangeable Graph Model}
[TODO]

\subsection{$p1$ Graph Model}
[TODO]

\subsection{$p2$ Graph Model}
[TODO]

\subsection{Stochastic Block Graph Model}
[TODO]

\subsection{Latent Space Graph Model}
[TODO]

\subsection{Exponential Random Graphs (ERGMs)}
[TODO]

\begin{remark}[Relation to Spectral Clustering]
[TODO]
\end{remark}


% % % % % Clustering % % % % % %

\section{Cluster Analysis}
\label{sec:cluster_analysis}

In cluster analysis, or \emph{data segmentation}, we aim at assigning observations to (hopefully) homogenous and meaningful clusters. 
We may also consider the more ambitious goal of finding not only clusters, but a \emph{clustering function}: a mapping between $\featureS$ to clusters (and not only between $X$ to clusters).

Cluster analysis is typically easier than learning a joint distribution (\S\ref{sec:density_estimation}) or even detecting high density regions (\S\ref{sec:high_density}). 

We will see that for cluster analysis, we don't always need the actual features $X$; it will turn out that many methods only require the pair-wise similarities ($\similaritys$). 
For this reason, clustering is intimately related to graph, or \emph{network}\footnote{A graph describes a yes/no relation. A network is a weighted graph, which not only describes the existence of a relation, but also its strength.} partitioning problems.



\subsection{K-Means Clustering}
\label{sec:kmeans}

\id{Clustering}{Algorithm}{$X, K$}{Data Clustering}{--}{--}


The goal behind K-means clustering algorithm is finding a representative point for each of K clusters, and assign each data point to one of these clusters. 
As each cluster has a representative point, this is also a \emph{prototype method}.\marginnote{Prototype Methods}.
The clusters are defined so that they minimize the average distance between all points to the center of the cluster.

K-means clustering requires the raw features $X$ as inputs, and not only a similarity graph. 
This is evident when examining Algorithm~\ref{algo:kmeans}.

In K-means, the clusters are first defined, and then similarities computed. This is thus a \emph{top-down} method.\marginnote{Top Down Clustering}

\begin{algorithm}[H]
\caption{K-Means}
\label{algo:kmeans}
\begin{algorithmic}
\State Choose the number of clusters $K$.
\State Arbitrarily assign points to clusters.
\While {Clusters keep changing}
	\State Compute the cluster centers as the average of their points.
	\State Assign each point to its closest cluster center (in Euclidean distance).
\EndWhile
\State \Return Cluster assignments and means.
\end{algorithmic}
\end{algorithm}


\begin{remark}[The population equivalent of K-means]
You may wonder- what population quantity is K-means actually estimating?
The estimand of K-means is known as the \emph{K principal points}.\marginnote{Principal Points}
Principal points are points which are \emph{self consistent}, i.e., they are the mean of their neighbourhood. 
\end{remark}




\subsection{K-Medoids Clustering (PAM)}
\label{sec:k_medoids}

\id{Clustering}{Algorithm}{$\similaritys, K$}{Data Clustering}{--}{--}


If a Euclidean distance is inappropriate for a particular $\featureS$, or that robustness to corrupt observations is required, or that we wish to constrain the cluster centers to be actual observations, then the \emph{K-Medoids} algorithm is an adaptation of K-means that allows this.
It is also known under the name \emph{partition around medoids}, or PAM, clustering.\marginnote{PAM clustering}

\begin{algorithm}[H]
\caption{K-Medoids}
\begin{algorithmic}
\State Given a similarity graph $\similaritys$.
\State Choose the number of clusters $K$.
\State Arbitrarily assign points to clusters.
\While {Clusters keep changing}
	\State Within each cluster, set the center as the data point that minimizes the sum of distances to other points in the cluster.
	\State Assign each point to its closest cluster center (in $\similarity(x_i,x_j)$ distance).
\EndWhile
\State \Return Cluster assignments and centers.
\end{algorithmic}
\end{algorithm}


See Section 14.3.10 in \cite{hastie_elements_2003}.




\subsection{Quality Threshold Clustering (QT)}
[TODO]
\label{sec:qt_clustering}



\subsection{Hierarchical Clustering}
\label{sec:hierarchical}

\id{Clustering}{Algorithm}{$\similaritys$}{Data Clustering}{--}{--}


These algorithms take similarity (dissimilarity) graphs as inputs.
Hierarchical clustering is a class of greedy graph-partitioning algorithms. 
Being hierarchical by design, they have the attractive property that the evolution of the clustering can be presented with a dendogram (\S\ref{sec:dendogram}).  
Unlike k-means, the method is the algorithm itself and cannot be cast as an optimization (ERM) problem.
Also, it does not require an a-priori choice of the number of cluster.

For more on hierarchical clustering see Section 14.3.12 in \cite{hastie_elements_2003}.

Two main sub-classes of algorithms can be identified: \emph{agglomerative}, and \emph{divisive}.
\subsubsection{Agglomerative Clustering}
Agglomerative clustering algorithms are bottom-up algorithm which build clusters by joining smaller clusters. 
To decide which clusters are joined at each iteration some measure of closeness between clusters is required. 
\begin{description}
\item[Single Linkage] Cluster distance is defined by the distance between the two \textbf{closest} members.
\item[Complete Linkage] Cluster distance is defined by the distance between the two \textbf{farthest} members.
\item[Group Average] Cluster distance is defined by the \textbf{average} distance between members.
\end{description}


[TODO: add plot of agglomerative clustering]

\subsubsection{Divisive Clustering}
Divisive clustering algorithms are top-down algorithm which build clusters by splitting larger clusters. 
There are several way to divide clusters. All amount to considering some homogeneity measure on the set of all possible divisions. As this is typically computationally intense, greedy algorithms can be employed.


[TODO: add plot of divisive clustering]




\subsection{Fuzzy Clustering}
\label{sec:fuzzy_clustering}
[TODO]




\subsection{Self Organizing Maps (SOM)}
\label{sec:som}

\id{Clustering}{Algorithm}{$X$}{Embedding function}{Dim. Reduce}{--}


SOMs, also known as \emph{Kohonen maps}, and \emph{self organizing feature maps} (SOFM), and \emph{constrained topological map}. 
They are dimensionality reduction methods driven by the quality of clustering.
If compared to MDS (\S\ref{sec:mds}), we can say that MDS tries to preserve the geometry of the data (i.e. distances) when embedding and SOMs try to conserve the topology (i.e. neighbours).
This is an advantage when clustering, but a disadvantage with visualizing. 

Unlike many similarity based dimensionality reduction methods, it requires the original features $X$, and not only a similarity graph $\similaritys$. \marginnote{Kohonen Map}

SOMs is an algorithm. It cannot be cast as an optimization problem. [TODO: verify]
It can be seen as the marriage of K-means clustering (\S\ref{sec:kmeans}) with a non-linear space embedding (\S\ref{sec:dim_reduce_nonlinear}). 
The surface approximating the data is a polygon mesh: typically a rectangle grid or a hexagon grid.
The ``dimension`` of the surface is governed by the density of the grid.

SOMs have the advantage that they learn an embedding function. Having learned an approximating surface, new data points can easily be embedded.
SOMs are also very efficient algorithmically, so that they may applied to very large data sets.

Note that SOMs can also be used for supervised learning. Either as a feature pre-processing stage, or in the actual learning.
Using SOMs as a feature pre-processing stage, may be known as \emph{counter-propagation network} [TODO: add reference ``kohonen: Self- and Super-organizing Maps in R``.]\marginnote{Counter-Propagation Network}


\paragraph{Terminology}
Some SOM related terms:
\begin{itemize}
\item \textbf{Codebook}: A parametrization of the approximating surface (grid). Uses a two-integer index for each neighbourhood.
\item \textbf{Segments plot}: A plot enabling the localization of the codebook units (approximating polygons) in the original space. Shows the coordinate of the unit's average.
\item \textbf{Counts plot}: Depicts the number of data points in each grid element.
\item \textbf{Quality plot}: Depicts the (average) distance between the a grid-element's centre and the associated data points. Smaller distances mean better approximations.
\item \textbf{Neighbour Distance Plot}: Depicts the (average) distance between the a grid-element and its immediate neighbours. Larger distances mean clearer clustering. Also known as \emph{U-matrix plot}.\marginnote{U-Matrix Plot}
\item \textbf{Property Plot}: Depicts the (average) value of a particular variable in the original space.
\end{itemize}

For more on SOMs see Section 14.4 in \cite{hastie_elements_2003}. 





\subsection{Spectral Clustering}
\label{sec:spectral_clustering}

\id{Clustering}{Meta-Algorithm}{$\similaritys$}{Data Clustering}{Spectral Trick}{--}


Spectral clustering is tailored for situations where \naive similarity measures $\similaritys$, such as the empirical covariances used in PCA, fail to capture the true notion of distances in the data, as it lay in some non convex set. 

For some intuition, let $X$ be $n$ images of facial expressions of the same individual. It would be quite miraculous if the Euclidean distance were to capture the true notion of distance between facial expressions. 

Spectral clustering implies following (meta-)algorithm:
\begin{algorithm}[H]
\caption{Spectral Clustering}
\begin{algorithmic}
\State Compute a similarity graph, $\similaritys$ with some \emph{local} similarity measure.
\State Convert the similarity graph $\similaritys$ to a dissimilarity graph $\dissimilaritys$ called the \emph{graph-laplacian}.\marginnote{Graph Laplacian}
\State Use the spectral trick (\S\ref{apx:spectral}) to reduce the dimension of the data.
\State Cluster data (say, with K-means, \S\ref{sec:kmeans} ) in the low-dimension representation. 
\State \Return Low dimensional data representation and cluster assignments. 
\end{algorithmic}
\end{algorithm}



The idea of using \emph{local} measures of similarity re-appears in several dimensionality reduction techniques such as LocalMDS (\S\ref{sec:localMDS}), Isomap (\S\ref{sec:isomap}) and LLE (\S\ref{sec:lle}).
Perhaps the most popular method is that of \emph{mutual K-nearest-neighbor graph}.\marginnote{Mutual K-Nearest-Neighbor Graph}

The spectral trick (Appendix \ref{apx:spectral}) is not a new one, and is motivated by the solution to the PCA problem (\S\ref{sec:pca}). In PCA, the solution is given by using the empirical covariances as similarities. 

The idea of clustering in a low dimensional representation of the data is a powerful one. 
Spectral clustering is a bundle of an embedding and clustering. 
These two, however, can be decoupled: just perform your favourite dimensionality reduction then followed by your favourite clustering technique.
SOMs (\S\ref{sec:som}), and exploratory projection pursuit (\S\ref{sec:exploratpory_ppr}) should be noted as techniques where the dimensionality reduction and the clustering \emph{cannot} be decoupled. In both, the dimensionality reduction is driven explicitly by the clustering performance.  


\subsection{Bi-Clustering}
[TODO]
\label{sec:bi_clustering}