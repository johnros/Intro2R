---
title: "Supervised Learning"
author: "Jonathan Rosenblatt"
date: "April 12, 2015"
output: 
  html_document:
    toc: true
---
In these examples, I will use two data sets from the `ElemStatLearn` package: `spam` for categorical predictions (spam or not spam?), and `prostate` for continuous predictions (size of cancerous tumor).
In `spam` we will try to decide if a mail is spam or not. 
In `prostate` we will try to predict the size of a cancerous tumor.

```{r}
# install.packages(ElemStatLearn)
library(ElemStatLearn)
data("prostate")
data("spam")
```
You can now call `?prostate` and `?spam` to learn more about these data sets.

We also load some utility packages and functions that we will require down the road. 
```{r preamble}
library(magrittr) # for piping
library(dplyr) # for handeling data frames

# My own utility functions:
l2 <- function(x) x^2 %>% sum %>% sqrt 
l1 <- function(x) abs(x) %>% sum  
MSE <- function(x) x^2 %>% mean 
missclassification <- function(tab) sum(tab[c(2,3)])/sum(tab)
```


# OLS

## OLS Regression

Starting with OLS regression, and a split train-test data set:
```{r OLS Regression}
prostate %>% names

prostate.train <- prostate %>% filter(train) %>% select(-train)
prostate.test <- prostate %>% filter(!train) %>% select(-train)
# now verify that your data looks as you would expect....

ols.1 <- lm(lcavol~. ,data = prostate.train)
# Train error:
MSE( predict(ols.1)- prostate.train$lcavol)
# Test error:
MSE( predict(ols.1, newdata = prostate.test)- prostate.test$lcavol)
```

Now using cross validation to estimate the prediction error:
```{r Cross Validation}
folds <- 5
fold.assignment <- sample(1:5, nrow(prostate), replace = TRUE)
errors <- NULL

for (k in 1:folds){
  prostate.cross.train <- prostate[fold.assignment!=k,]
  prostate.cross.test <-  prostate[fold.assignment==k,] 
  .ols <- lm(lcavol~. ,data = prostate.cross.train)
  .predictions <- predict(.ols, newdata=prostate.cross.test)
  .errors <-  .predictions - prostate.cross.test$lcavol
  errors <- c(errors, .errors)
}

# Cross validated prediction error:
MSE(errors)
```

Also trying a bootstrap prediction error:
```{r Bootstrap}
B <- 20
n <- nrow(prostate)
errors <- NULL

prostate.boot.test <-  prostate 
for (b in 1:B){
  prostate.boot.train <- prostate[sample(1:n, replace = TRUE),]
  .ols <- lm(lcavol~. ,data = prostate.boot.train)
  .predictions <- predict(.ols, newdata=prostate.boot.test)
  .errors <-  .predictions - prostate.boot.test$lcavol
  errors <- c(errors, .errors)
}

# Bootstrapped prediction error:
MSE(errors)
```


### OLS Regression Model Selection 
Trying Bootstrapped, Cross Validated, and AIC.

AIC model selection: 
```{r OLS AIC}
step(ols.1, scope=model.scope, direction='backward', trace = TRUE)

ols.0 <- lm(lcavol~1 ,data = prostate.train)
model.scope <- list(upper=ols.1, lower=ols.0)
step(ols.0, scope=model.scope, direction='forward', trace = TRUE)
```


Cross Validated Model Selection.
```{r OLS CV}

```


Bootstrap model selection:
```{r OLS bootstrap}



```




## OLS Classification
```{r OLS Classification}
names(spam)
n <- nrow(spam)
spam.dummy <- spam %>% mutate(spam=as.numeric(spam=='spam'))

# Making train and test sets:
train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>% 
  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam.dummy[train.ind,]
spam.test <- spam.dummy[!train.ind,]

ols.2 <- lm(spam~., data = spam.train)

# Train confusion matrix:
.predictions.train <- predict(ols.2) > 0.5
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(ols.2, newdata = spam.test) > 0.5
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```



# Ridge Regression

```{r Ridge I}
library(ridge)

ridge.1 <- linearRidge(lcavol~. ,data = prostate.train)
# Note that if not specified, lambda is chosen automatically by linearRidge.

# Train error:
MSE( predict(ridge.1)- prostate.train$lcavol)
# Test error:
MSE( predict(ridge.1, newdata = prostate.test)- prostate.test$lcavol)
```

Another implementation, which also automatically chooses the tuning parameter $\lambda$:
```{r Ridge II}
library(glmnet)
y.train <- prostate.train$lcavol
X.train <- prostate.train %>% select(-lcavol) %>% as.matrix
ridge.2 <- glmnet(x=X.train, y=y.train, alpha = 0)

# Train error:
MSE( predict(ridge.2, newx =X.train)- y.train)

# Test error:
y.test <- prostate.test$lcavol 
X.test <- prostate.test %>% select(-lcavol) %>% as.matrix

MSE( predict(ridge.2, newx = X.test)- y.test)
```

__Note__:  `glmnet` is slightly picky: I could not have created `y.train` using `select()` because I need a vector and not a `data.frame`. Also, `as.matrix` is there as `glmnet` expects a `matrix` class `x` argument.




# LASSO Regression
```{r LASSO}
library(glmnet)
y.train <- prostate.train$lcavol
X.train <- prostate.train %>% select(-lcavol) %>% as.matrix
lasso.1 <- glmnet(x=X.train, y=y.train, alpha = 1)

# Train error:
MSE( predict(lasso.1, newx =X.train)- y.train)

# Test error:
y.test <- prostate.test$lcavol
X.test <- prostate.test %>% select(-lcavol) %>% as.matrix

MSE( predict(lasso.1, newx = X.test)- y.test)
```


# Logistic Regression For Classification
```{r Logistic Regression}
# Making train and test sets:
train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>%  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam[train.ind,]
spam.test <- spam[!train.ind,]

logistic.1 <- glm(spam~., data = spam.train, family = binomial)
# numerical error. Probably due to too many predictors. 
# Maybe regularizing the logistic regressio with Ridge or LASSO will make things better?
```

In the next chunk, we do $l_2$ and $l_1$ regularized logistic regression.
Some technical remarks are in order:

- `glmnet` is picky with its inputs. This has already been discussed in the context of the LASSO regression above.
- The `predict` function for `glmnet` objects returns a prediction (see below) for many candidate  regularization levels $\lambda$. We thus we `cv.glmnet` which does an automatic cross validated selection of the best regularization level. 
```{r Regularized Logistic Regression}
library(glmnet)
y.train <- spam.train$spam
X.train <- spam.train %>% select(-spam) %>% as.matrix
# Ridge Regularization with CV selection of regularization:
logistic.2 <- cv.glmnet(x=X.train, y=y.train, family = "binomial", alpha = 0)
# LASSO Regularization with CV selection of regularization:
logistic.3 <- cv.glmnet(x=X.train, y=y.train, family = "binomial", alpha = 1)


# Train confusion matrix:
.predictions.train <- predict(logistic.2, newx = X.train, type = 'class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

.predictions.train <- predict(logistic.3, newx = X.train, type = 'class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
y.test <- spam.test$spam
X.test <- spam.test %>% select(-spam) %>% as.matrix


.predictions.test <- predict(logistic.2, newx = X.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=y.test))
missclassification(confusion.test)

.predictions.test <- predict(logistic.3, newx = X.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=y.test))
missclassification(confusion.test)
```




# SVM

## Classification
```{r SVM classification}
train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>%  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam[train.ind,]
spam.test <- spam[!train.ind,]

library(e1071)
svm.1 <- svm(spam~., data = spam.train)

# Train confusion matrix:
.predictions.train <- predict(svm.1) 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(svm.1, newdata = spam.test) 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```


## Regression
```{r SVM regression}
prostate.train <- prostate %>% filter(train) %>% select(-train)
prostate.test <- prostate %>% filter(!train) %>% select(-train)

svm.2 <- svm(lcavol~., data = prostate.train)

# Train error:
MSE( predict(svm.2)- prostate.train$lcavol)
# Test error:
MSE( predict(svm.2, newdata = prostate.test)- prostate.test$lcavol)
```




# GAM Regression
```{r GAM}
# install.packages('mgcv')
library(mgcv)
form.1 <- lcavol~ s(lweight)+ s(age)+s(lbph)+s(svi)+s(lcp)+s(gleason)+s(pgg45)+s(lpsa)
gam.1 <- gam(form.1, data = prostate.train)

sort(abs(coef(ridge.1)), decreasing = TRUE)
form.2 <- lcavol~  s(lweight)+ s(age)+s(lbph)+s(lcp)+s(pgg45)+s(lpsa)
gam.2 <- gam(form.2, data = prostate.train)

# Train error:
MSE( predict(gam.2)- prostate.train$lcavol)
# Test error:
MSE( predict(gam.2, newdata = prostate.test)- prostate.test$lcavol)
```


# Neural Net

## Regression
```{r NNET regression}
library(nnet)
nnet.1 <- nnet(lcavol~., size=20, data=prostate.train, rang = 0.1, decay = 5e-4, maxit = 1000)

# Train error:
MSE( predict(nnet.1)- prostate.train$lcavol)
# Test error:
MSE( predict(nnet.1, newdata = prostate.test)- prostate.test$lcavol)
```

Let's automate the network size selection:
```{r NNET validate}
validate.nnet <- function(size){
  .nnet <- nnet(lcavol~., size=size, data=prostate.train, rang = 0.1, decay = 5e-4, maxit = 200)
  .train <- MSE( predict(.nnet)- prostate.train$lcavol)
  .test <- MSE( predict(.nnet, newdata = prostate.test)- prostate.test$lcavol)
  return(list(train=.train, test=.test))
}

validate.nnet(3)
validate.nnet(4)
validate.nnet(20)
validate.nnet(50)

sizes <- seq(2, 30)
validate.sizes <- rep(NA, length(sizes))
for (i in seq_along(sizes)){
  validate.sizes[i] <- validate.nnet(sizes[i])$test
}
plot(validate.sizes~sizes, type='l')
```
What can I say... This plot is not what I would expect. Could be due to the random nature of the fitting algorithm.



## Classification
```{r NNET Classification}
train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>%  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam[train.ind,]
spam.test <- spam[!train.ind,]
nnet.2 <- nnet(spam~., size=5, data=spam.train, rang = 0.1, decay = 5e-4, maxit = 1000)

# Train confusion matrix:
.predictions.train <- predict(nnet.2, type='class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(nnet.2, newdata = spam.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```


# CART

## Regression
```{r Tree regression}
library(rpart)
tree.1 <- rpart(lcavol~., data=prostate.train)

# Train error:
MSE( predict(tree.1)- prostate.train$lcavol)
# Test error:
MSE( predict(tree.1, newdata = prostate.test)- prostate.test$lcavol)
```

At this stage we should prune the tree using `prune()`...

## Classification
```{r Tree classification}
train.prop <- 0.66
train.ind <- c(TRUE,FALSE) %>%  sample(size = n, prob = c(train.prop,1-train.prop), replace=TRUE)
spam.train <- spam[train.ind,]
spam.test <- spam[!train.ind,]

tree.2 <- rpart(spam~., data=spam.train)

# Train confusion matrix:
.predictions.train <- predict(tree.2, type='class') 
(confusion.train <- table(prediction=.predictions.train, truth=spam.train$spam))
missclassification(confusion.train)

# Test confusion matrix:
.predictions.test <- predict(tree.2, newdata = spam.test, type='class') 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

# Smoothing Splines
I will demonstrate the method with a single predictor, so that we can visualize the smoothing that has been performed:

```{r Smoothing Splines}
y.train <- prostate.train$lcavol
X.train <- prostate.train$age

spline.1 <- smooth.spline(x=X.train, y=y.train)

# Visualize the non linear hypothesis we have learned:
plot(y.train~X.train, col='red', type='h')
points(spline.1, type='l')
```
I am not extracting train and test errors as the output of `smooth.spline` will require some tweaking for that.


# Knn

## Classification
```{r knn classification}
library(class)
y.train <- spam.train$spam
X.train <- spam.train %>% select(-spam) %>% as.matrix
y.test <- spam.test$spam
X.test <- spam.test %>% select(-spam) %>% as.matrix

knn.1 <- knn(train = X.train, test = X.test, cl =y.train, k = 1)

# Test confusion matrix:
.predictions.test <- knn.1 
(confusion.test <- table(prediction=.predictions.test, truth=spam.test$spam))
missclassification(confusion.test)
```

And now we would try to optimize `k` by trying different values.


# Kernel Regression
Kernel regression includes many particular algorithms. 
```{r kernel}
install.packages('np')
library(np)

y.train <- prostate.train$lcavol
X.train <- prostate.train %>% select(-lcavol) %>% as.matrix

ksmooth.1 <- npreg(txdat =X.train, tydat = y.train)

# Train error:
MSE( predict(ksmooth.1)- prostate.train$lcavol)
```

There is currently no method to make prediction on test data with this function.



# Stacking
As seen in the class notes, there are many ensemble methods.
Stacking, in my view, is by far the most useful and coolest. It is thus the only one I present here.

In this example, I will stack an OLS, a Tree, and a GAM.

```{r Stacking}
[TODO]
```


